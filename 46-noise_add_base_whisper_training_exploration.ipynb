{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyB0fe7yF4Mh"
   },
   "source": [
    "# Fine-tuning Whisper on Speech Pathology Dataset\n",
    "\n",
    "## Goal\n",
    "\n",
    "The goal of the Cleft Palate project (name TBD) at Vanderbilt DSI is to classify audio clips of patients' voices as containing hypernasality (a speech impediment) or not. The patients with hypernasality can then be recommended for speech pathology intervention. This is currently evaluated by human speech pathologists, which requires access to these medical providers. Our hope is to train a model that can classify this speech impediment for expedited patient access to a speech pathologist.\n",
    "\n",
    "Tutorial created with guidance from [\"Fine Tuning OpenAI Whisper Model for Audio Classifcation in PyTorch\"](https://www.daniweb.com/programming/computer-science/tutorials/540802/fine-tuning-openai-whisper-model-for-audio-classification-in-pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-jy3au5F4Mi"
   },
   "source": [
    "## Model\n",
    "\n",
    "We plan to use the Whisper embedings from OpenAI and train a classification model, either using Whisper with a sequence classification head or another classification LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZrXyRlPF4Mi"
   },
   "source": [
    "## Data\n",
    "\n",
    "The data in this notebook is publicly available voice recordings featuring hypernasality and control groups. In the future we hope to train our model on private patient data from Vanderbilt University Medical Center (VUMC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCbmzVnlF4Mi"
   },
   "source": [
    "### Split Data\n",
    "\n",
    "We need to split our data into train and test sets, then save those for further experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KifWdFjsF-B6",
    "outputId": "8ba11edd-aac0-4645-890c-471aea2c3025"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.0a0+81ea7a4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2023.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets)\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m252.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m255.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m267.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
      "Successfully installed datasets-2.18.0 dill-0.3.8 huggingface-hub-0.22.2 multiprocess-0.70.16 xxhash-3.4.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.0)\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.57.1+1.g4157f3379)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.0)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.8.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.7)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.40.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (4.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (23.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (2.31.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2023.11.17)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.39.2-py3-none-any.whl.metadata (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
      "Downloading transformers-4.39.2-py3-none-any.whl (8.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m278.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m169.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, tokenizers, transformers\n",
      "Successfully installed safetensors-0.4.2 tokenizers-0.15.2 transformers-4.39.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install datasets\n",
    "!pip install librosa\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "0frbPwrsF4Mj"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import datasets\n",
    "from datasets import load_dataset, DatasetDict,  Audio\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import librosa\n",
    "import io\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "from transformers import WhisperModel, WhisperFeatureExtractor, AdamW\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/cleft_palate_choja'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "_UNczqVkMCGU"
   },
   "outputs": [],
   "source": [
    "data_path = \"/workspace/cleft_palate_choja/WAV_PUBLIC_SAMPLES/NOISE\"\n",
    "\n",
    "train_catalog = \"/workspace/cleft_palate_choja/train_noise.csv\"\n",
    "test_catalog = \"/workspace/cleft_palate_choja/test_noise.csv\"\n",
    "\n",
    "model_checkpoint = \"openai/whisper-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "cK7hP1fMMnZB",
    "outputId": "75b5a900-3735-47de-9b09-98e7131ffa18"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Name</th>\n",
       "      <th>Sampling_Rate_(Hz)</th>\n",
       "      <th>Channels</th>\n",
       "      <th>Duration_(seconds)</th>\n",
       "      <th>folder</th>\n",
       "      <th>hypernasality</th>\n",
       "      <th>original_text</th>\n",
       "      <th>OPENAI_Whisper_text</th>\n",
       "      <th>WAV_filename</th>\n",
       "      <th>WAV_folder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACPA ted had a dog with white feet-3.mp3</td>\n",
       "      <td>44100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.13</td>\n",
       "      <td>CASES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ted had a dog with white feet</td>\n",
       "      <td>Ted and a dog with white feet.</td>\n",
       "      <td>ACPA ted had a dog with white feet-3.wav</td>\n",
       "      <td>CASES_WAV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cdc 4 (and then go to school).mp3</td>\n",
       "      <td>44100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.41</td>\n",
       "      <td>CONTROLS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>and then go to school</td>\n",
       "      <td>and then go to school.</td>\n",
       "      <td>cdc 4 (and then go to school).wav</td>\n",
       "      <td>CONTROLS_WAV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Video 1_4 (and can I have some more material).mp3</td>\n",
       "      <td>44100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.60</td>\n",
       "      <td>CONTROLS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>and can I have some more material</td>\n",
       "      <td>And can I have some more material?</td>\n",
       "      <td>Video 1_4 (and can I have some more material).wav</td>\n",
       "      <td>CONTROLS_WAV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEW - video 2 (three times).mp3</td>\n",
       "      <td>44100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>CONTROLS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>three times</td>\n",
       "      <td>Three times.</td>\n",
       "      <td>NEW - video 2 (three times).wav</td>\n",
       "      <td>CONTROLS_WAV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cdc 4 (and then he brushed his teeth).mp3</td>\n",
       "      <td>44100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.52</td>\n",
       "      <td>CONTROLS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>and then he brushed his teeth</td>\n",
       "      <td>And then he brushed his teeth.</td>\n",
       "      <td>cdc 4 (and then he brushed his teeth).wav</td>\n",
       "      <td>CONTROLS_WAV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>video 1 (pizza bundt).mp3</td>\n",
       "      <td>44100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>CONTROLS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pizza bundt</td>\n",
       "      <td>Pizza Funt!</td>\n",
       "      <td>NOISE-video 1 (pizza bundt).wav</td>\n",
       "      <td>CONTROLS_WAV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>ACPA most boys like to play football-3.mp3</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.31</td>\n",
       "      <td>CASES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>most boys like to play football</td>\n",
       "      <td>Most boys like to play football.</td>\n",
       "      <td>NOISE-ACPA most boys like to play football-3.wav</td>\n",
       "      <td>CASES_WAV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>Facebook  (take a tire).mp3</td>\n",
       "      <td>44100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.75</td>\n",
       "      <td>CASES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>take a tire</td>\n",
       "      <td>See you next time!</td>\n",
       "      <td>NOISE-Facebook  (take a tire).wav</td>\n",
       "      <td>CASES_WAV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>Video 5_1 (feet).mp3</td>\n",
       "      <td>44100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.04</td>\n",
       "      <td>CASES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>feet</td>\n",
       "      <td>Peace.</td>\n",
       "      <td>NOISE-Video 5_1 (feet).wav</td>\n",
       "      <td>CASES_WAV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>Video 1_7 (here_s some pizza).mp3</td>\n",
       "      <td>44100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.12</td>\n",
       "      <td>CONTROLS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>here's some pizza</td>\n",
       "      <td>Here's your pizza.</td>\n",
       "      <td>NOISE-Video 1_7 (here_s some pizza).wav</td>\n",
       "      <td>CONTROLS_WAV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>294 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             File_Name  Sampling_Rate_(Hz)  \\\n",
       "0             ACPA ted had a dog with white feet-3.mp3             44100.0   \n",
       "1                    cdc 4 (and then go to school).mp3             44100.0   \n",
       "2    Video 1_4 (and can I have some more material).mp3             44100.0   \n",
       "3                      NEW - video 2 (three times).mp3             44100.0   \n",
       "4            cdc 4 (and then he brushed his teeth).mp3             44100.0   \n",
       "..                                                 ...                 ...   \n",
       "289                          video 1 (pizza bundt).mp3             44100.0   \n",
       "290         ACPA most boys like to play football-3.mp3             48000.0   \n",
       "291                        Facebook  (take a tire).mp3             44100.0   \n",
       "292                               Video 5_1 (feet).mp3             44100.0   \n",
       "293                  Video 1_7 (here_s some pizza).mp3             44100.0   \n",
       "\n",
       "     Channels  Duration_(seconds)    folder  hypernasality  \\\n",
       "0         1.0                4.13     CASES            1.0   \n",
       "1         2.0                1.41  CONTROLS            0.0   \n",
       "2         2.0                3.60  CONTROLS            0.0   \n",
       "3         2.0                1.28  CONTROLS            0.0   \n",
       "4         2.0                1.52  CONTROLS            0.0   \n",
       "..        ...                 ...       ...            ...   \n",
       "289       2.0                1.80  CONTROLS            0.0   \n",
       "290       1.0                3.31     CASES            1.0   \n",
       "291       1.0                1.75     CASES            1.0   \n",
       "292       2.0                1.04     CASES            1.0   \n",
       "293       2.0                2.12  CONTROLS            0.0   \n",
       "\n",
       "                         original_text                  OPENAI_Whisper_text  \\\n",
       "0        ted had a dog with white feet      Ted and a dog with white feet.    \n",
       "1                and then go to school              and then go to school.    \n",
       "2    and can I have some more material  And can I have some more material?    \n",
       "3                          three times                        Three times.    \n",
       "4        and then he brushed his teeth      And then he brushed his teeth.    \n",
       "..                                 ...                                  ...   \n",
       "289                        pizza bundt                         Pizza Funt!    \n",
       "290    most boys like to play football    Most boys like to play football.    \n",
       "291                        take a tire                  See you next time!    \n",
       "292                               feet                              Peace.    \n",
       "293                  here's some pizza                  Here's your pizza.    \n",
       "\n",
       "                                          WAV_filename    WAV_folder  \n",
       "0             ACPA ted had a dog with white feet-3.wav     CASES_WAV  \n",
       "1                    cdc 4 (and then go to school).wav  CONTROLS_WAV  \n",
       "2    Video 1_4 (and can I have some more material).wav  CONTROLS_WAV  \n",
       "3                      NEW - video 2 (three times).wav  CONTROLS_WAV  \n",
       "4            cdc 4 (and then he brushed his teeth).wav  CONTROLS_WAV  \n",
       "..                                                 ...           ...  \n",
       "289                    NOISE-video 1 (pizza bundt).wav  CONTROLS_WAV  \n",
       "290   NOISE-ACPA most boys like to play football-3.wav     CASES_WAV  \n",
       "291                  NOISE-Facebook  (take a tire).wav     CASES_WAV  \n",
       "292                         NOISE-Video 5_1 (feet).wav     CASES_WAV  \n",
       "293            NOISE-Video 1_7 (here_s some pizza).wav  CONTROLS_WAV  \n",
       "\n",
       "[294 rows x 10 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metadata = pd.read_csv(train_catalog)\n",
    "train_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "1nK-GGDSGnwF"
   },
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(train_metadata, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "asMFxjOFShao"
   },
   "outputs": [],
   "source": [
    "train_files = train_df[\"WAV_filename\"].tolist()\n",
    "\n",
    "train_folder = train_df[\"WAV_folder\"].tolist()\n",
    "\n",
    "train_full_paths = [os.path.join(data_path,train_folder[i], train_files[i]) for i in range(0,len(train_files))]\n",
    "\n",
    "#train_full_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8H7EPlZcTkvs",
    "outputId": "34b27042-68c4-4f9a-fff4-1a4bfbde6a78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = train_df[\"hypernasality\"].tolist()\n",
    "\n",
    "train_labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "wR_oKNmxHVB5"
   },
   "outputs": [],
   "source": [
    "# val set\n",
    "val_files = val_df[\"WAV_filename\"].tolist()\n",
    "\n",
    "val_folder = val_df[\"WAV_folder\"].tolist()\n",
    "\n",
    "val_full_paths = [os.path.join(data_path,val_folder[i], val_files[i]) for i in range(0,len(val_files))]\n",
    "\n",
    "val_labels = val_df[\"hypernasality\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0nVArYbmHfuV",
    "outputId": "950e7737-6bee-4e24-b24c-8c243b1e3101"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "bSZZHxQcUmTi"
   },
   "outputs": [],
   "source": [
    "test_metadata = pd.read_csv(test_catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l2dxxV6vUsO2",
    "outputId": "3dc81a6a-fd3e-47da-d186-dc08b9a7348e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_511/3166376184.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test_metadata['WAV_filename'] = test_metadata['File_Name'].str.replace('.mp3', '.wav')\n"
     ]
    }
   ],
   "source": [
    "# add cols for wav data\n",
    "\n",
    "# Replace \".mp3\" with \".wav\" in the \"Filename\" column\n",
    "test_metadata['WAV_filename'] = test_metadata['File_Name'].str.replace('.mp3', '.wav')\n",
    "\n",
    "# Create \"WAV_folder\" column by concatenating \"_WAV\" to the \"folder\" column\n",
    "test_metadata['WAV_folder'] = test_metadata['folder'] + \"_WAV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "gg--b6ufULkK"
   },
   "outputs": [],
   "source": [
    "test_files = test_metadata[\"WAV_filename\"].tolist()\n",
    "\n",
    "test_folder = test_metadata[\"WAV_folder\"].tolist()\n",
    "\n",
    "test_full_paths = [os.path.join(data_path,test_folder[i], test_files[i]) for i in range(0,len(test_files))]\n",
    "\n",
    "#test_full_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "NrzkUkYXVJbT"
   },
   "outputs": [],
   "source": [
    "test_labels = test_metadata[\"hypernasality\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSk_DTreF4Ml"
   },
   "source": [
    "### Create PyTorch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "wQs-OauiF4Ml"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_audio_dataset = datasets.Dataset.from_dict({\"audio\": train_full_paths,\n",
    "                                                  \"labels\":train_labels}\n",
    "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "\n",
    "test_audio_dataset = datasets.Dataset.from_dict({\"audio\": test_full_paths,\n",
    "                                                  \"labels\": test_labels}\n",
    "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "\n",
    "val_audio_dataset = datasets.Dataset.from_dict({\"audio\": val_full_paths,\n",
    "                                                 \"labels\": val_labels }\n",
    "                                             ).cast_column(\"audio\", Audio(sampling_rate=16_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217,
     "referenced_widgets": [
      "8cb9edfc75434d14b838b412c6810196",
      "a10c743f1fb14855b87d4be1776624ba",
      "6f7a9ecfc73446f3aed7264cca40b513",
      "8b44bf9bd55c4510b13b679850000fc2",
      "73f88ec9be55418495b8a6d18caaf309",
      "6b161a029f4a4319bc03486aa6095418",
      "4f3cb021985744b0ad19ae7cf39ca1a9",
      "cc031250814b4b80a623ed770d26501f",
      "fea45e76479747c782c1501e7fb2df40",
      "3b988137321345ab9d5053fa3bb1584a",
      "6de39f1a49684c6b9e4e6f590a753167",
      "8aa0332a573949248a31446990eeea8b",
      "8a0a9b46f37d45bd8fe9f25cfad4c2e6",
      "d9981e0dd6934118b82dc7ee3af2b151",
      "d88980a81c9846b4b29536cef16f7431",
      "c2e0c28aa38148ce8cda7959ce4e9a31",
      "bf9e4d0c7c7d42f490ec00cb01388780",
      "f26c492caee849109f16a8739a094478",
      "97ef9e98800047c9928b7e818df16628",
      "520c0a3e5b904aa1b253af945977bbac",
      "e29f8e6a1aa242fd8693bcf6db07908f",
      "75680f71cc9b4c50aa5d037c11a5d4cb",
      "262f00dd95404da08656c09e11343d5a",
      "741790bd41ed49da82cc65329d5420cf",
      "b9dc8ca1c66e4ed8813fea0ee6b5013c",
      "edf13b3438b94ad4a46801585ce2e7ed",
      "cb4e2ce75af74eefa2cc9dea57a28af6",
      "be214894409c41dba66907d2e8bbbe3f",
      "730678f8b7b94787a92baff44328b967",
      "fb3e295db229449da0f1db045ee8ab9d",
      "a9771fb56dda4e85a03903528a99db23",
      "3ecfecbb0c564add9c7e0d934d36cf4f",
      "40d34a6291c1425bb189d39786a1d9e3"
     ]
    },
    "id": "cC0SCQsZF4Mm",
    "outputId": "ed536510-ce9f-4552-f744-a91c34ad4030"
   },
   "outputs": [],
   "source": [
    "#model_checkpoint = \"openai/whisper-base\"\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "encoder = WhisperModel.from_pretrained(model_checkpoint)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "09pDF62uF4Mm"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SpeechClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, audio_data,  text_processor):\n",
    "        self.audio_data = audio_data\n",
    "        self.text_processor = text_processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "      inputs = self.text_processor(self.audio_data[index][\"audio\"][\"array\"],\n",
    "                                   return_tensors=\"pt\",\n",
    "                                   sampling_rate=self.audio_data[index][\"audio\"][\"sampling_rate\"])\n",
    "      input_features = inputs.input_features\n",
    "      decoder_input_ids = torch.tensor([[1, 1]]) * encoder.config.decoder_start_token_id\n",
    "\n",
    "      labels = np.array(self.audio_data[index]['labels'])\n",
    "\n",
    "      return input_features, decoder_input_ids, torch.tensor(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "5gw-7620F4Mm"
   },
   "outputs": [],
   "source": [
    "train_dataset = SpeechClassificationDataset(train_audio_dataset,  feature_extractor)\n",
    "test_dataset = SpeechClassificationDataset(test_audio_dataset,  feature_extractor)\n",
    "val_dataset = SpeechClassificationDataset(val_audio_dataset,  feature_extractor)\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzARqHUEF4Mm"
   },
   "source": [
    "## Fine Tune Whisper Model\n",
    "\n",
    "Whisper model from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "JVjXzUnbF4Mm"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SpeechClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, encoder):\n",
    "        super(SpeechClassifier, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.encoder.config.hidden_size, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_features, decoder_input_ids):\n",
    "        outputs = self.encoder(input_features, decoder_input_ids=decoder_input_ids)\n",
    "        pooled_output = outputs['last_hidden_state'][:, 0, :]\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VIkVorEXF4Mn",
    "outputId": "855bc5f2-9fb9-4d7c-f56e-2bdf3bfa242b"
   },
   "outputs": [],
   "source": [
    "num_labels = 2\n",
    "\n",
    "model = SpeechClassifier(num_labels, encoder).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-08)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "3uioy2R1V1Gn"
   },
   "outputs": [],
   "source": [
    "# Define the training function NO VAL\n",
    "def train(model, train_loader, optimizer, criterion, device, num_epochs):\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "\n",
    "          input_features, decoder_input_ids, labels = batch\n",
    "\n",
    "          input_features = input_features.squeeze()\n",
    "          input_features = input_features.to(device)\n",
    "\n",
    "          decoder_input_ids = decoder_input_ids.squeeze()\n",
    "          decoder_input_ids = decoder_input_ids.to(device)\n",
    "\n",
    "          labels = labels.view(-1)\n",
    "          labels = labels.type(torch.LongTensor)\n",
    "          labels = labels.to(device)\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          logits = model(input_features, decoder_input_ids)\n",
    "\n",
    "          loss = criterion(logits, labels)\n",
    "          loss.backward()\n",
    "\n",
    "          optimizer.step()\n",
    "\n",
    "          if (i+1) % 8 == 0:\n",
    "              print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item():.4f}')\n",
    "\n",
    "    torch.save(model.state_dict(), 'best_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "ziIJ-x0BF4Mn"
   },
   "outputs": [],
   "source": [
    "# Define the training function\n",
    "def train(model, train_loader, val_loader, optimizer,  criterion, device, num_epochs):\n",
    "    best_accuracy = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            input_features, decoder_input_ids, labels = batch\n",
    "            input_features = input_features.squeeze()\n",
    "            input_features = input_features.to(device)\n",
    "            decoder_input_ids = decoder_input_ids.squeeze()\n",
    "            decoder_input_ids = decoder_input_ids.to(device)\n",
    "            labels = labels.view(-1)\n",
    "            labels = labels.type(torch.LongTensor)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_features, decoder_input_ids)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (i+1) % 8 == 0:\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item() :.4f}')\n",
    "                train_loss = 0.0\n",
    "        val_loss, val_accuracy, val_f1, _ , _ = evaluate(model, val_loader, device)\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "        print(\"========================================================================================\")\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}, Best Accuracy: {best_accuracy:.4f}')\n",
    "        print(\"========================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "Z7Q4-ze0F4Mn"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader,  device):\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "          input_features, decoder_input_ids, labels = batch\n",
    "          input_features = input_features.squeeze()\n",
    "          input_features = input_features.to(device)\n",
    "          decoder_input_ids = decoder_input_ids.squeeze()\n",
    "          decoder_input_ids = decoder_input_ids.to(device)\n",
    "          labels = labels.view(-1)\n",
    "          labels = labels.type(torch.LongTensor)\n",
    "          labels = labels.to(device)\n",
    "          optimizer.zero_grad()\n",
    "          logits = model(input_features, decoder_input_ids)\n",
    "          loss = criterion(logits, labels)\n",
    "          total_loss += loss.item()\n",
    "          _, preds = torch.max(logits, 1)\n",
    "          all_labels.append(labels.cpu().numpy())\n",
    "          all_preds.append(preds.cpu().numpy())\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    loss = total_loss / len(data_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    return loss, accuracy, f1, all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IUv6LcFNF4Mn",
    "outputId": "f92a0109-8d1a-4cf2-89e7-cbf354b1d859"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Batch 8/41, Train Loss: 0.6397\n",
      "Epoch 1/5, Batch 16/41, Train Loss: 0.4986\n",
      "Epoch 1/5, Batch 24/41, Train Loss: 0.2008\n",
      "Epoch 1/5, Batch 32/41, Train Loss: 0.2401\n",
      "Epoch 1/5, Batch 40/41, Train Loss: 0.2498\n",
      "========================================================================================\n",
      "Epoch 1/5, Val Loss: 0.1442, Val Accuracy: 0.9551, Val F1: 0.9549, Best Accuracy: 0.9551\n",
      "========================================================================================\n",
      "Epoch 2/5, Batch 8/41, Train Loss: 0.0223\n",
      "Epoch 2/5, Batch 16/41, Train Loss: 0.4859\n",
      "Epoch 2/5, Batch 24/41, Train Loss: 0.0414\n",
      "Epoch 2/5, Batch 32/41, Train Loss: 0.0206\n",
      "Epoch 2/5, Batch 40/41, Train Loss: 0.0325\n",
      "========================================================================================\n",
      "Epoch 2/5, Val Loss: 0.1103, Val Accuracy: 0.9551, Val F1: 0.9549, Best Accuracy: 0.9551\n",
      "========================================================================================\n",
      "Epoch 3/5, Batch 8/41, Train Loss: 0.0023\n",
      "Epoch 3/5, Batch 16/41, Train Loss: 0.0025\n",
      "Epoch 3/5, Batch 24/41, Train Loss: 0.0088\n",
      "Epoch 3/5, Batch 32/41, Train Loss: 0.0349\n",
      "Epoch 3/5, Batch 40/41, Train Loss: 0.0020\n",
      "========================================================================================\n",
      "Epoch 3/5, Val Loss: 0.1116, Val Accuracy: 0.9775, Val F1: 0.9775, Best Accuracy: 0.9775\n",
      "========================================================================================\n",
      "Epoch 4/5, Batch 8/41, Train Loss: 0.0039\n",
      "Epoch 4/5, Batch 16/41, Train Loss: 0.0025\n",
      "Epoch 4/5, Batch 24/41, Train Loss: 0.0006\n",
      "Epoch 4/5, Batch 32/41, Train Loss: 0.0007\n",
      "Epoch 4/5, Batch 40/41, Train Loss: 0.0004\n",
      "========================================================================================\n",
      "Epoch 4/5, Val Loss: 0.0618, Val Accuracy: 0.9775, Val F1: 0.9773, Best Accuracy: 0.9775\n",
      "========================================================================================\n",
      "Epoch 5/5, Batch 8/41, Train Loss: 0.0001\n",
      "Epoch 5/5, Batch 16/41, Train Loss: 0.0002\n",
      "Epoch 5/5, Batch 24/41, Train Loss: 0.0003\n",
      "Epoch 5/5, Batch 32/41, Train Loss: 0.0001\n",
      "Epoch 5/5, Batch 40/41, Train Loss: 0.0005\n",
      "========================================================================================\n",
      "Epoch 5/5, Val Loss: 0.0652, Val Accuracy: 0.9775, Val F1: 0.9773, Best Accuracy: 0.9775\n",
      "========================================================================================\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "num_epochs = 5\n",
    "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjETS9sqYRD5"
   },
   "source": [
    "### Validation\n",
    "\n",
    "Before running the model on the test set, let's examine the validation set and see how our model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "bILZRSn4JvlG"
   },
   "outputs": [],
   "source": [
    "#VALIDATION\n",
    "state_dict = torch.load('best_model.pt')\n",
    "\n",
    "# Create a new instance of the model and load the state dictionary\n",
    "num_labels = 2\n",
    "model = SpeechClassifier(num_labels, encoder).to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "_, _, _, all_labels, all_preds = evaluate(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fyNUpLHeJ5p1",
    "outputId": "52379010-0e14-4cd5-eaa7-bd858a033532"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98        41\n",
      "           1       1.00      0.96      0.98        48\n",
      "\n",
      "    accuracy                           0.98        89\n",
      "   macro avg       0.98      0.98      0.98        89\n",
      "weighted avg       0.98      0.98      0.98        89\n",
      "\n",
      "0.9775280898876404\n"
     ]
    }
   ],
   "source": [
    "#VALIDATION\n",
    "print(classification_report(all_labels, all_preds))\n",
    "print(accuracy_score(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l56wgPY1Qja3"
   },
   "source": [
    "This is too good to be true. Checking the contents of labels, preds, and data balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nefkmyAVLDRO",
    "outputId": "00198469-6838-4cdd-a6bb-5557a59470a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "       0])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AL-CcSn0LGgk",
    "outputId": "8c2e1758-da55-4632-f796-9246548c4e2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "       0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "       0])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I6GfSmERLMA4",
    "outputId": "7bedfc59-523c-4fab-943f-8bdead50feac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5170731707317073"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_labels)/len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aiaKvCpnLRrd",
    "outputId": "5b48efc7-21b5-4ea5-d180-ce46a74bb6fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5393258426966292"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(val_labels)/len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "BBh8OAXvR7Qr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      1.00      0.84        36\n",
      "           1       1.00      0.63      0.77        38\n",
      "\n",
      "    accuracy                           0.81        74\n",
      "   macro avg       0.86      0.82      0.81        74\n",
      "weighted avg       0.86      0.81      0.80        74\n",
      "\n",
      "0.8108108108108109\n"
     ]
    }
   ],
   "source": [
    "# TESTING ONLY\n",
    "state_dict = torch.load('best_model.pt')\n",
    "\n",
    "# Create a new instance of the model and load the state dictionary\n",
    "num_labels = 2\n",
    "model = SpeechClassifier(num_labels, encoder).to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "_, _, _, all_labels, all_preds = evaluate(model, test_loader, device)\n",
    "\n",
    "\n",
    "print(classification_report(all_labels, all_preds))\n",
    "print(accuracy_score(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_d_MMju6R71D"
   },
   "source": [
    "I don't want to run testing yet as we want to explore more models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Quoc3-z4YaJx"
   },
   "source": [
    "### Model Troubleshooting\n",
    "\n",
    "So far our results look too good to be true (98% validation accuracy). In the cells below I run through some troubleshooting methods to ensure our model is not overfit or learning the wrong representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JoYDBxOQzJt"
   },
   "source": [
    "Ensure that the labels are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PDwMkIKvNLCv",
    "outputId": "762468da-693b-40a1-9e4e-9b02213b073f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118    0.0\n",
       "216    0.0\n",
       "18     0.0\n",
       "193    0.0\n",
       "176    0.0\n",
       "      ... \n",
       "121    0.0\n",
       "293    0.0\n",
       "20     0.0\n",
       "188    0.0\n",
       "270    0.0\n",
       "Name: hypernasality, Length: 99, dtype: float64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df[\"WAV_folder\"] == \"CONTROLS_WAV\"][\"hypernasality\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 843
    },
    "id": "SnNHVlCdNafl",
    "outputId": "17e73924-6d48-4490-9561-875e9fb01440"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Name</th>\n",
       "      <th>Sampling_Rate_(Hz)</th>\n",
       "      <th>Channels</th>\n",
       "      <th>Duration_(seconds)</th>\n",
       "      <th>folder</th>\n",
       "      <th>hypernasality</th>\n",
       "      <th>original_text</th>\n",
       "      <th>OPENAI_Whisper_text</th>\n",
       "      <th>WAV_filename</th>\n",
       "      <th>WAV_folder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>video 1 (four).mp3</td>\n",
       "      <td>44100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.04</td>\n",
       "      <td>CONTROLS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>four</td>\n",
       "      <td>Voila!</td>\n",
       "      <td>video 1 (four).wav</td>\n",
       "      <td>CONTROLS_WAV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>ACPA sue roasted a duck for supper.mp3</td>\n",
       "      <td>44100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.59</td>\n",
       "      <td>CONTROLS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>sue roasted a duck for supper</td>\n",
       "      <td>Sue roasted a duck for supper.</td>\n",
       "      <td>NOISE-ACPA sue roasted a duck for supper.wav</td>\n",
       "      <td>CONTROLS_WAV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>NEW - video 7 (puppy).mp3</td>\n",
       "      <td>44100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.91</td>\n",
       "      <td>CASES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>puppy</td>\n",
       "      <td>Fuck me!</td>\n",
       "      <td>NEW - video 7 (puppy).wav</td>\n",
       "      <td>CASES_WAV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>cdc 4 (and then he was a boy).mp3</td>\n",
       "      <td>44100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.57</td>\n",
       "      <td>CONTROLS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>and then he was a boy</td>\n",
       "      <td>And then he was a boy.</td>\n",
       "      <td>cdc 4 (and then he was a boy).wav</td>\n",
       "      <td>CONTROLS_WAV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>ACPA Tom had ham and eggs for breakfast-2.mp3</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.65</td>\n",
       "      <td>CASES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Tom had ham and eggs for breakfast</td>\n",
       "      <td>Tom has ham and eggs for breakfast.</td>\n",
       "      <td>NOISE-ACPA Tom had ham and eggs for breakfast-...</td>\n",
       "      <td>CASES_WAV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>Video 1_3 (its all of our birthdays).mp3</td>\n",
       "      <td>44100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.78</td>\n",
       "      <td>CONTROLS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>its all of our birthdays</td>\n",
       "      <td>It's Alma's birthday.</td>\n",
       "      <td>NOISE-Video 1_3 (its all of our birthdays).wav</td>\n",
       "      <td>CONTROLS_WAV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Video 4_4 (well it will help me).mp3</td>\n",
       "      <td>44100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.32</td>\n",
       "      <td>CASES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>well it will help me</td>\n",
       "      <td>Wow, em vừa học đĩa</td>\n",
       "      <td>Video 4_4 (well it will help me).wav</td>\n",
       "      <td>CASES_WAV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>ACPA buy baby a bib.mp3</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.92</td>\n",
       "      <td>CASES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>buy baby a bib</td>\n",
       "      <td>Hi, I'm Hayley Mim.</td>\n",
       "      <td>ACPA buy baby a bib.wav</td>\n",
       "      <td>CASES_WAV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>cdc 3 (he won_t fly away).mp3</td>\n",
       "      <td>44100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.14</td>\n",
       "      <td>CONTROLS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>he won't fly away</td>\n",
       "      <td>He won't fly away!</td>\n",
       "      <td>NOISE-cdc 3 (he won_t fly away).wav</td>\n",
       "      <td>CONTROLS_WAV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>ACPA we shouldn_t play in the street-2.mp3</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.16</td>\n",
       "      <td>CASES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>we shouldn't play in the street</td>\n",
       "      <td>I shouldn't play with food.</td>\n",
       "      <td>ACPA we shouldn_t play in the street-2.wav</td>\n",
       "      <td>CASES_WAV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>205 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         File_Name  Sampling_Rate_(Hz)  \\\n",
       "118                             video 1 (four).mp3             44100.0   \n",
       "216         ACPA sue roasted a duck for supper.mp3             44100.0   \n",
       "97                       NEW - video 7 (puppy).mp3             44100.0   \n",
       "18               cdc 4 (and then he was a boy).mp3             44100.0   \n",
       "170  ACPA Tom had ham and eggs for breakfast-2.mp3             48000.0   \n",
       "..                                             ...                 ...   \n",
       "188       Video 1_3 (its all of our birthdays).mp3             44100.0   \n",
       "71            Video 4_4 (well it will help me).mp3             44100.0   \n",
       "106                        ACPA buy baby a bib.mp3             48000.0   \n",
       "270                  cdc 3 (he won_t fly away).mp3             44100.0   \n",
       "102     ACPA we shouldn_t play in the street-2.mp3             48000.0   \n",
       "\n",
       "     Channels  Duration_(seconds)    folder  hypernasality  \\\n",
       "118       2.0                1.04  CONTROLS            0.0   \n",
       "216       1.0                2.59  CONTROLS            0.0   \n",
       "97        2.0                0.91     CASES            1.0   \n",
       "18        2.0                1.57  CONTROLS            0.0   \n",
       "170       1.0                3.65     CASES            1.0   \n",
       "..        ...                 ...       ...            ...   \n",
       "188       2.0                1.78  CONTROLS            0.0   \n",
       "71        2.0                2.32     CASES            1.0   \n",
       "106       1.0                1.92     CASES            1.0   \n",
       "270       2.0                2.14  CONTROLS            0.0   \n",
       "102       1.0                2.16     CASES            1.0   \n",
       "\n",
       "                          original_text                   OPENAI_Whisper_text  \\\n",
       "118                                four                               Voila!    \n",
       "216       sue roasted a duck for supper       Sue roasted a duck for supper.    \n",
       "97                                puppy                             Fuck me!    \n",
       "18                and then he was a boy               And then he was a boy.    \n",
       "170  Tom had ham and eggs for breakfast  Tom has ham and eggs for breakfast.    \n",
       "..                                  ...                                   ...   \n",
       "188            its all of our birthdays                It's Alma's birthday.    \n",
       "71                 well it will help me                  Wow, em vừa học đĩa    \n",
       "106                      buy baby a bib                  Hi, I'm Hayley Mim.    \n",
       "270                   he won't fly away                   He won't fly away!    \n",
       "102     we shouldn't play in the street          I shouldn't play with food.    \n",
       "\n",
       "                                          WAV_filename    WAV_folder  \n",
       "118                                 video 1 (four).wav  CONTROLS_WAV  \n",
       "216       NOISE-ACPA sue roasted a duck for supper.wav  CONTROLS_WAV  \n",
       "97                           NEW - video 7 (puppy).wav     CASES_WAV  \n",
       "18                   cdc 4 (and then he was a boy).wav  CONTROLS_WAV  \n",
       "170  NOISE-ACPA Tom had ham and eggs for breakfast-...     CASES_WAV  \n",
       "..                                                 ...           ...  \n",
       "188     NOISE-Video 1_3 (its all of our birthdays).wav  CONTROLS_WAV  \n",
       "71                Video 4_4 (well it will help me).wav     CASES_WAV  \n",
       "106                            ACPA buy baby a bib.wav     CASES_WAV  \n",
       "270                NOISE-cdc 3 (he won_t fly away).wav  CONTROLS_WAV  \n",
       "102         ACPA we shouldn_t play in the street-2.wav     CASES_WAV  \n",
       "\n",
       "[205 rows x 10 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvLayklhYpSe"
   },
   "source": [
    "Making a dummy label set to make sure that my model isn't taking random guesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "ez1PM8qwNDsC"
   },
   "outputs": [],
   "source": [
    "# dummy data\n",
    "import random\n",
    "\n",
    "# Define the length of the list you want\n",
    "length = len(train_labels)  # Change this to your desired length\n",
    "\n",
    "# Generate a list of random 1s and 0s of the specified length\n",
    "dummy_list = [random.choice([0, 1]) for _ in range(length)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "ATern9FEMwQF"
   },
   "outputs": [],
   "source": [
    "dummy_df = train_df\n",
    "dummy_df[\"DUMMY\"] = dummy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "fEN4yr5zMiIG"
   },
   "outputs": [],
   "source": [
    "dummy_audio_dataset = datasets.Dataset.from_dict({\"audio\": train_full_paths,\n",
    "                                                  \"labels\":dummy_list}\n",
    "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "\n",
    "dummy_dataset = SpeechClassificationDataset(dummy_audio_dataset,  feature_extractor)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "dummy_loader = DataLoader(dummy_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "2oPd_WTlPiba"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"openai/whisper-base\"\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "encoder = WhisperModel.from_pretrained(model_checkpoint)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "YTW6vdGLMBsY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "num_labels = 2\n",
    "\n",
    "model = SpeechClassifier(num_labels, encoder).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-08)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ds1BqzP5OuIE",
    "outputId": "d1abf7ef-6cbb-464f-c2f3-1a4487f81161"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Batch 8/26, Train Loss: 0.7364\n",
      "Epoch 1/5, Batch 16/26, Train Loss: 0.6733\n",
      "Epoch 1/5, Batch 24/26, Train Loss: 0.6919\n",
      "========================================================================================\n",
      "Epoch 1/5, Val Loss: 0.7062, Val Accuracy: 0.4719, Val F1: 0.4652, Best Accuracy: 0.4719\n",
      "========================================================================================\n",
      "Epoch 2/5, Batch 8/26, Train Loss: 0.7694\n",
      "Epoch 2/5, Batch 16/26, Train Loss: 0.6981\n",
      "Epoch 2/5, Batch 24/26, Train Loss: 0.5655\n",
      "========================================================================================\n",
      "Epoch 2/5, Val Loss: 0.8443, Val Accuracy: 0.4045, Val F1: 0.3791, Best Accuracy: 0.4719\n",
      "========================================================================================\n",
      "Epoch 3/5, Batch 8/26, Train Loss: 0.2857\n",
      "Epoch 3/5, Batch 16/26, Train Loss: 0.4488\n",
      "Epoch 3/5, Batch 24/26, Train Loss: 0.6563\n",
      "========================================================================================\n",
      "Epoch 3/5, Val Loss: 1.1812, Val Accuracy: 0.5506, Val F1: 0.4137, Best Accuracy: 0.5506\n",
      "========================================================================================\n",
      "Epoch 4/5, Batch 8/26, Train Loss: 0.1893\n",
      "Epoch 4/5, Batch 16/26, Train Loss: 0.2174\n",
      "Epoch 4/5, Batch 24/26, Train Loss: 0.5401\n",
      "========================================================================================\n",
      "Epoch 4/5, Val Loss: 1.0358, Val Accuracy: 0.4719, Val F1: 0.4621, Best Accuracy: 0.5506\n",
      "========================================================================================\n",
      "Epoch 5/5, Batch 8/26, Train Loss: 0.3944\n",
      "Epoch 5/5, Batch 16/26, Train Loss: 0.0689\n",
      "Epoch 5/5, Batch 24/26, Train Loss: 0.0494\n",
      "========================================================================================\n",
      "Epoch 5/5, Val Loss: 0.9385, Val Accuracy: 0.4944, Val F1: 0.4615, Best Accuracy: 0.5506\n",
      "========================================================================================\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "train(model, dummy_loader, val_loader, optimizer, criterion, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F40v4RGmRAoK"
   },
   "source": [
    "Model is not learning with the dummy data...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGSvxTq2VbAo"
   },
   "source": [
    "## Simpler Model\n",
    "\n",
    "Let's train a simpler model to see how our model does compared to a simpler one such as SVM or Random Forrest. Generated with help from ChatGPT4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GoL6qAD3XPZa"
   },
   "source": [
    "### SVM\n",
    "\n",
    "Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FNt0KvLKVhqC",
    "outputId": "851e631a-3fb4-4e0f-8257-dd2c1f1a381c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7796610169491526\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.87      0.80        30\n",
      "         1.0       0.83      0.69      0.75        29\n",
      "\n",
      "    accuracy                           0.78        59\n",
      "   macro avg       0.79      0.78      0.78        59\n",
      "weighted avg       0.79      0.78      0.78        59\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Define a function to extract MFCCs from an audio file\n",
    "def extract_mfcc_features(file_path, n_mfcc=13):\n",
    "    audio, sample_rate = librosa.load(file_path, sr=None)\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)\n",
    "    mfccs_scaled = np.mean(mfccs.T, axis=0)  # Taking the average across time\n",
    "    return mfccs_scaled\n",
    "\n",
    "# Paths to your audio files (replace these with your actual file paths)\n",
    "audio_files = train_full_paths + test_full_paths  # Add more paths as needed\n",
    "labels = train_labels + test_labels  # Corresponding labels for your audio files\n",
    "\n",
    "# Extract features from each audio file\n",
    "features = [extract_mfcc_features(file) for file in audio_files]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(x_train)\n",
    "X_test = scaler.transform(x_test)\n",
    "\n",
    "# Initialize and train the SVM classifier\n",
    "svm_model = SVC(kernel='linear')  # You can experiment with different kernels\n",
    "svm_model.fit(x_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = svm_model.predict(x_val)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"Classification Report:\", classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gch3m5CvXTBW"
   },
   "source": [
    "### Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EtACXySZXU2d",
    "outputId": "5b5fa04a-6fd2-4bb0-f056-e3a62d1854ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.847457627118644\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.90      0.86        30\n",
      "         1.0       0.88      0.79      0.84        29\n",
      "\n",
      "    accuracy                           0.85        59\n",
      "   macro avg       0.85      0.85      0.85        59\n",
      "weighted avg       0.85      0.85      0.85        59\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Initialize and train the Random Forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100)  # You can adjust the number of trees\n",
    "rf_model.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions - VAL\n",
    "y_pred = rf_model.predict(x_val)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"Classification Report:\", classification_report(y_val, y_pred))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "262f00dd95404da08656c09e11343d5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_741790bd41ed49da82cc65329d5420cf",
       "IPY_MODEL_b9dc8ca1c66e4ed8813fea0ee6b5013c",
       "IPY_MODEL_edf13b3438b94ad4a46801585ce2e7ed"
      ],
      "layout": "IPY_MODEL_cb4e2ce75af74eefa2cc9dea57a28af6"
     }
    },
    "3b988137321345ab9d5053fa3bb1584a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ecfecbb0c564add9c7e0d934d36cf4f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40d34a6291c1425bb189d39786a1d9e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4f3cb021985744b0ad19ae7cf39ca1a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "520c0a3e5b904aa1b253af945977bbac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6b161a029f4a4319bc03486aa6095418": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6de39f1a49684c6b9e4e6f590a753167": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6f7a9ecfc73446f3aed7264cca40b513": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cc031250814b4b80a623ed770d26501f",
      "max": 184990,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fea45e76479747c782c1501e7fb2df40",
      "value": 184990
     }
    },
    "730678f8b7b94787a92baff44328b967": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "73f88ec9be55418495b8a6d18caaf309": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "741790bd41ed49da82cc65329d5420cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be214894409c41dba66907d2e8bbbe3f",
      "placeholder": "​",
      "style": "IPY_MODEL_730678f8b7b94787a92baff44328b967",
      "value": "model.safetensors: 100%"
     }
    },
    "75680f71cc9b4c50aa5d037c11a5d4cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8a0a9b46f37d45bd8fe9f25cfad4c2e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bf9e4d0c7c7d42f490ec00cb01388780",
      "placeholder": "​",
      "style": "IPY_MODEL_f26c492caee849109f16a8739a094478",
      "value": "config.json: 100%"
     }
    },
    "8aa0332a573949248a31446990eeea8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8a0a9b46f37d45bd8fe9f25cfad4c2e6",
       "IPY_MODEL_d9981e0dd6934118b82dc7ee3af2b151",
       "IPY_MODEL_d88980a81c9846b4b29536cef16f7431"
      ],
      "layout": "IPY_MODEL_c2e0c28aa38148ce8cda7959ce4e9a31"
     }
    },
    "8b44bf9bd55c4510b13b679850000fc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b988137321345ab9d5053fa3bb1584a",
      "placeholder": "​",
      "style": "IPY_MODEL_6de39f1a49684c6b9e4e6f590a753167",
      "value": " 185k/185k [00:00&lt;00:00, 430kB/s]"
     }
    },
    "8cb9edfc75434d14b838b412c6810196": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a10c743f1fb14855b87d4be1776624ba",
       "IPY_MODEL_6f7a9ecfc73446f3aed7264cca40b513",
       "IPY_MODEL_8b44bf9bd55c4510b13b679850000fc2"
      ],
      "layout": "IPY_MODEL_73f88ec9be55418495b8a6d18caaf309"
     }
    },
    "97ef9e98800047c9928b7e818df16628": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a10c743f1fb14855b87d4be1776624ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6b161a029f4a4319bc03486aa6095418",
      "placeholder": "​",
      "style": "IPY_MODEL_4f3cb021985744b0ad19ae7cf39ca1a9",
      "value": "preprocessor_config.json: 100%"
     }
    },
    "a9771fb56dda4e85a03903528a99db23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b9dc8ca1c66e4ed8813fea0ee6b5013c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb3e295db229449da0f1db045ee8ab9d",
      "max": 290403936,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a9771fb56dda4e85a03903528a99db23",
      "value": 290403936
     }
    },
    "be214894409c41dba66907d2e8bbbe3f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf9e4d0c7c7d42f490ec00cb01388780": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2e0c28aa38148ce8cda7959ce4e9a31": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb4e2ce75af74eefa2cc9dea57a28af6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc031250814b4b80a623ed770d26501f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d88980a81c9846b4b29536cef16f7431": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e29f8e6a1aa242fd8693bcf6db07908f",
      "placeholder": "​",
      "style": "IPY_MODEL_75680f71cc9b4c50aa5d037c11a5d4cb",
      "value": " 1.98k/1.98k [00:00&lt;00:00, 131kB/s]"
     }
    },
    "d9981e0dd6934118b82dc7ee3af2b151": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_97ef9e98800047c9928b7e818df16628",
      "max": 1983,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_520c0a3e5b904aa1b253af945977bbac",
      "value": 1983
     }
    },
    "e29f8e6a1aa242fd8693bcf6db07908f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "edf13b3438b94ad4a46801585ce2e7ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ecfecbb0c564add9c7e0d934d36cf4f",
      "placeholder": "​",
      "style": "IPY_MODEL_40d34a6291c1425bb189d39786a1d9e3",
      "value": " 290M/290M [00:04&lt;00:00, 44.9MB/s]"
     }
    },
    "f26c492caee849109f16a8739a094478": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fb3e295db229449da0f1db045ee8ab9d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fea45e76479747c782c1501e7fb2df40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
