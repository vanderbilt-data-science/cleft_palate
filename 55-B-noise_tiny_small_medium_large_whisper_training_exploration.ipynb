{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyB0fe7yF4Mh"
   },
   "source": [
    "# Fine-tuning Whisper on Speech Pathology Dataset\n",
    "\n",
    "## Goal\n",
    "\n",
    "The goal of the Cleft Palate project at Vanderbilt DSI is to classify audio clips of patients' voices as containing hypernasality (a speech impediment) or not. The patients with hypernasality can then be recommended for speech pathology intervention. This is currently evaluated by human speech pathologists, which requires access to these medical providers. Our hope is to train a model that can classify this speech impediment for expedited patient access to a speech pathologist.\n",
    "\n",
    "Tutorial created with guidance from [\"Fine Tuning OpenAI Whisper Model for Audio Classifcation in PyTorch\"](https://www.daniweb.com/programming/computer-science/tutorials/540802/fine-tuning-openai-whisper-model-for-audio-classification-in-pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-jy3au5F4Mi"
   },
   "source": [
    "## Model\n",
    "\n",
    "We plan to use the Whisper embedings from OpenAI and train a classification model, either using Whisper with a sequence classification head or another classification LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZrXyRlPF4Mi"
   },
   "source": [
    "## Data\n",
    "\n",
    "The data in this notebook is publicly available voice recordings featuring hypernasality and control groups. In the future we hope to train our model on private patient data from Vanderbilt University Medical Center (VUMC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCbmzVnlF4Mi"
   },
   "source": [
    "### Split Data\n",
    "\n",
    "We need to split our data into train and test sets, then save those for further experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KifWdFjsF-B6",
    "outputId": "8ba11edd-aac0-4645-890c-471aea2c3025"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.0a0+81ea7a4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.22.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.0)\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.57.1+1.g4157f3379)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.0)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.8.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.7)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.40.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (4.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (23.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (2.31.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2023.11.17)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install datasets\n",
    "!pip install librosa\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0frbPwrsF4Mj"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import datasets\n",
    "from datasets import load_dataset, DatasetDict,  Audio\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import librosa\n",
    "import io\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "from transformers import WhisperModel, WhisperFeatureExtractor, AdamW\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==================================================================================================================\n",
    "# Tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_UNczqVkMCGU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1499/2197773112.py:26: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test_metadata['WAV_filename'] = test_metadata['File_Name'].str.replace('.mp3', '.wav')\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Batch 8/41, Train Loss: 1.1287\n",
      "Epoch 1/5, Batch 16/41, Train Loss: 0.8537\n",
      "Epoch 1/5, Batch 24/41, Train Loss: 0.7160\n",
      "Epoch 1/5, Batch 32/41, Train Loss: 0.5143\n",
      "Epoch 1/5, Batch 40/41, Train Loss: 0.3436\n",
      "========================================================================================\n",
      "Epoch 1/5, Val Loss: 0.2688, Val Accuracy: 0.8652, Val F1: 0.8650, Best Accuracy: 0.8652\n",
      "========================================================================================\n",
      "Epoch 2/5, Batch 8/41, Train Loss: 0.4391\n",
      "Epoch 2/5, Batch 16/41, Train Loss: 0.0375\n",
      "Epoch 2/5, Batch 24/41, Train Loss: 0.0240\n",
      "Epoch 2/5, Batch 32/41, Train Loss: 0.0628\n",
      "Epoch 2/5, Batch 40/41, Train Loss: 0.0135\n",
      "========================================================================================\n",
      "Epoch 2/5, Val Loss: 0.0816, Val Accuracy: 0.9663, Val F1: 0.9662, Best Accuracy: 0.9663\n",
      "========================================================================================\n",
      "Epoch 3/5, Batch 8/41, Train Loss: 0.0014\n",
      "Epoch 3/5, Batch 16/41, Train Loss: 0.0028\n",
      "Epoch 3/5, Batch 24/41, Train Loss: 0.0171\n",
      "Epoch 3/5, Batch 32/41, Train Loss: 0.0019\n",
      "Epoch 3/5, Batch 40/41, Train Loss: 0.0028\n",
      "========================================================================================\n",
      "Epoch 3/5, Val Loss: 0.0420, Val Accuracy: 0.9888, Val F1: 0.9887, Best Accuracy: 0.9888\n",
      "========================================================================================\n",
      "Epoch 4/5, Batch 8/41, Train Loss: 0.0010\n",
      "Epoch 4/5, Batch 16/41, Train Loss: 0.0048\n",
      "Epoch 4/5, Batch 24/41, Train Loss: 0.0033\n",
      "Epoch 4/5, Batch 32/41, Train Loss: 0.0013\n",
      "Epoch 4/5, Batch 40/41, Train Loss: 0.0028\n",
      "========================================================================================\n",
      "Epoch 4/5, Val Loss: 0.0446, Val Accuracy: 0.9888, Val F1: 0.9887, Best Accuracy: 0.9888\n",
      "========================================================================================\n",
      "Epoch 5/5, Batch 8/41, Train Loss: 0.0005\n",
      "Epoch 5/5, Batch 16/41, Train Loss: 0.0003\n",
      "Epoch 5/5, Batch 24/41, Train Loss: 0.0002\n",
      "Epoch 5/5, Batch 32/41, Train Loss: 0.0014\n",
      "Epoch 5/5, Batch 40/41, Train Loss: 0.0002\n",
      "========================================================================================\n",
      "Epoch 5/5, Val Loss: 0.0451, Val Accuracy: 0.9888, Val F1: 0.9887, Best Accuracy: 0.9888\n",
      "========================================================================================\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/workspace/cleft_palate_choja/WAV_PUBLIC_SAMPLES/NOISE\"\n",
    "\n",
    "train_catalog = \"/workspace/cleft_palate_choja/train_noise.csv\"\n",
    "test_catalog = \"/workspace/cleft_palate_choja/test_noise.csv\"\n",
    "model_checkpoint = \"openai/whisper-tiny\"\n",
    "\n",
    "train_metadata = pd.read_csv(train_catalog)\n",
    "train_df, val_df = train_test_split(train_metadata, test_size = 0.3, random_state = 42)\n",
    "train_files = train_df[\"WAV_filename\"].tolist()\n",
    "train_folder = train_df[\"WAV_folder\"].tolist()\n",
    "train_full_paths = [os.path.join(data_path,train_folder[i], train_files[i]) for i in range(0,len(train_files))]\n",
    "train_labels = train_df[\"hypernasality\"].tolist()\n",
    "# val set\n",
    "val_files = val_df[\"WAV_filename\"].tolist()\n",
    "\n",
    "val_folder = val_df[\"WAV_folder\"].tolist()\n",
    "\n",
    "val_full_paths = [os.path.join(data_path,val_folder[i], val_files[i]) for i in range(0,len(val_files))]\n",
    "\n",
    "val_labels = val_df[\"hypernasality\"].tolist()\n",
    "\n",
    "test_metadata = pd.read_csv(test_catalog)\n",
    "# add cols for wav data\n",
    "\n",
    "# Replace \".mp3\" with \".wav\" in the \"Filename\" column\n",
    "test_metadata['WAV_filename'] = test_metadata['File_Name'].str.replace('.mp3', '.wav')\n",
    "\n",
    "# Create \"WAV_folder\" column by concatenating \"_WAV\" to the \"folder\" column\n",
    "test_metadata['WAV_folder'] = test_metadata['folder'] + \"_WAV\"\n",
    "\n",
    "test_files = test_metadata[\"WAV_filename\"].tolist()\n",
    "\n",
    "test_folder = test_metadata[\"WAV_folder\"].tolist()\n",
    "\n",
    "test_full_paths = [os.path.join(data_path,test_folder[i], test_files[i]) for i in range(0,len(test_files))]\n",
    "\n",
    "#test_full_paths\n",
    "\n",
    "test_labels = test_metadata[\"hypernasality\"].tolist()\n",
    "\n",
    "train_audio_dataset = datasets.Dataset.from_dict({\"audio\": train_full_paths,\n",
    "                                                  \"labels\":train_labels}\n",
    "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "test_audio_dataset = datasets.Dataset.from_dict({\"audio\": test_full_paths,\n",
    "                                                  \"labels\": test_labels}\n",
    "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "val_audio_dataset = datasets.Dataset.from_dict({\"audio\": val_full_paths,\n",
    "                                                 \"labels\": val_labels }\n",
    "                                             ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "#model_checkpoint = \"openai/whisper-base\"\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "encoder = WhisperModel.from_pretrained(model_checkpoint)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class SpeechClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, audio_data,  text_processor):\n",
    "        self.audio_data = audio_data\n",
    "        self.text_processor = text_processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "      inputs = self.text_processor(self.audio_data[index][\"audio\"][\"array\"],\n",
    "                                   return_tensors=\"pt\",\n",
    "                                   sampling_rate=self.audio_data[index][\"audio\"][\"sampling_rate\"])\n",
    "      input_features = inputs.input_features\n",
    "      decoder_input_ids = torch.tensor([[1, 1]]) * encoder.config.decoder_start_token_id\n",
    "\n",
    "      labels = np.array(self.audio_data[index]['labels'])\n",
    "\n",
    "      return input_features, decoder_input_ids, torch.tensor(labels)\n",
    "train_dataset = SpeechClassificationDataset(train_audio_dataset,  feature_extractor)\n",
    "test_dataset = SpeechClassificationDataset(test_audio_dataset,  feature_extractor)\n",
    "val_dataset = SpeechClassificationDataset(val_audio_dataset,  feature_extractor)\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class SpeechClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, encoder):\n",
    "        super(SpeechClassifier, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.encoder.config.hidden_size, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_features, decoder_input_ids):\n",
    "        outputs = self.encoder(input_features, decoder_input_ids=decoder_input_ids)\n",
    "        pooled_output = outputs['last_hidden_state'][:, 0, :]\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "num_labels = 2\n",
    "\n",
    "model = SpeechClassifier(num_labels, encoder).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-08)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Define the training function NO VAL\n",
    "def train(model, train_loader, optimizer, criterion, device, num_epochs):\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "\n",
    "          input_features, decoder_input_ids, labels = batch\n",
    "\n",
    "          input_features = input_features.squeeze()\n",
    "          input_features = input_features.to(device)\n",
    "\n",
    "          decoder_input_ids = decoder_input_ids.squeeze()\n",
    "          decoder_input_ids = decoder_input_ids.to(device)\n",
    "\n",
    "          labels = labels.view(-1)\n",
    "          labels = labels.type(torch.LongTensor)\n",
    "          labels = labels.to(device)\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          logits = model(input_features, decoder_input_ids)\n",
    "\n",
    "          loss = criterion(logits, labels)\n",
    "          loss.backward()\n",
    "\n",
    "          optimizer.step()\n",
    "\n",
    "          if (i+1) % 8 == 0:\n",
    "              print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item():.4f}')\n",
    "\n",
    "    torch.save(model.state_dict(), 'best_model.pt')\n",
    "# Define the training function\n",
    "def train(model, train_loader, val_loader, optimizer,  criterion, device, num_epochs):\n",
    "    best_accuracy = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            input_features, decoder_input_ids, labels = batch\n",
    "            input_features = input_features.squeeze()\n",
    "            input_features = input_features.to(device)\n",
    "            decoder_input_ids = decoder_input_ids.squeeze()\n",
    "            decoder_input_ids = decoder_input_ids.to(device)\n",
    "            labels = labels.view(-1)\n",
    "            labels = labels.type(torch.LongTensor)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_features, decoder_input_ids)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (i+1) % 8 == 0:\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item() :.4f}')\n",
    "                train_loss = 0.0\n",
    "        val_loss, val_accuracy, val_f1, _ , _ = evaluate(model, val_loader, device)\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "        print(\"========================================================================================\")\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}, Best Accuracy: {best_accuracy:.4f}')\n",
    "        print(\"========================================================================================\")\n",
    "def evaluate(model, data_loader,  device):\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "          input_features, decoder_input_ids, labels = batch\n",
    "          input_features = input_features.squeeze()\n",
    "          input_features = input_features.to(device)\n",
    "          decoder_input_ids = decoder_input_ids.squeeze()\n",
    "          decoder_input_ids = decoder_input_ids.to(device)\n",
    "          labels = labels.view(-1)\n",
    "          labels = labels.type(torch.LongTensor)\n",
    "          labels = labels.to(device)\n",
    "          optimizer.zero_grad()\n",
    "          logits = model(input_features, decoder_input_ids)\n",
    "          loss = criterion(logits, labels)\n",
    "          total_loss += loss.item()\n",
    "          _, preds = torch.max(logits, 1)\n",
    "          all_labels.append(labels.cpu().numpy())\n",
    "          all_preds.append(preds.cpu().numpy())\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    loss = total_loss / len(data_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    return loss, accuracy, f1, all_labels, all_preds\n",
    "\n",
    "import librosa\n",
    "num_epochs = 5\n",
    "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjETS9sqYRD5"
   },
   "source": [
    "### Validation\n",
    "\n",
    "Before running the model on the test set, let's examine the validation set and see how our model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bILZRSn4JvlG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Validtation\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        41\n",
      "           1       0.98      1.00      0.99        48\n",
      "\n",
      "    accuracy                           0.99        89\n",
      "   macro avg       0.99      0.99      0.99        89\n",
      "weighted avg       0.99      0.99      0.99        89\n",
      "\n",
      "0.9887640449438202\n",
      "================\n",
      "Testing\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.89      0.91        36\n",
      "           1       0.90      0.95      0.92        38\n",
      "\n",
      "    accuracy                           0.92        74\n",
      "   macro avg       0.92      0.92      0.92        74\n",
      "weighted avg       0.92      0.92      0.92        74\n",
      "\n",
      "0.918918918918919\n"
     ]
    }
   ],
   "source": [
    "#VALIDATION\n",
    "state_dict = torch.load('best_model.pt')\n",
    "\n",
    "# Create a new instance of the model and load the state dictionary\n",
    "num_labels = 2\n",
    "model = SpeechClassifier(num_labels, encoder).to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "_, _, _, all_labels, all_preds = evaluate(model, val_loader, device)\n",
    "\n",
    "#VALIDATION\n",
    "print(\"================\")\n",
    "print(\"Validtation\\n\\n\")\n",
    "print(classification_report(all_labels, all_preds))\n",
    "print(accuracy_score(all_labels, all_preds))\n",
    "\n",
    "# TESTING ONLY\n",
    "state_dict = torch.load('best_model.pt')\n",
    "\n",
    "# Create a new instance of the model and load the state dictionary\n",
    "num_labels = 2\n",
    "model = SpeechClassifier(num_labels, encoder).to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "_, _, _, all_labels, all_preds = evaluate(model, test_loader, device)\n",
    "\n",
    "print(\"================\")\n",
    "print(\"Testing\\n\\n\")\n",
    "print(classification_report(all_labels, all_preds))\n",
    "print(accuracy_score(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==================================================================================================================\n",
    "# Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1499/3089817272.py:26: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test_metadata['WAV_filename'] = test_metadata['File_Name'].str.replace('.mp3', '.wav')\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Batch 8/41, Train Loss: 0.6626\n",
      "Epoch 1/5, Batch 16/41, Train Loss: 0.3709\n",
      "Epoch 1/5, Batch 24/41, Train Loss: 0.0620\n",
      "Epoch 1/5, Batch 32/41, Train Loss: 0.0348\n",
      "Epoch 1/5, Batch 40/41, Train Loss: 0.3284\n",
      "========================================================================================\n",
      "Epoch 1/5, Val Loss: 0.1450, Val Accuracy: 0.9326, Val F1: 0.9311, Best Accuracy: 0.9326\n",
      "========================================================================================\n",
      "Epoch 2/5, Batch 8/41, Train Loss: 0.0330\n",
      "Epoch 2/5, Batch 16/41, Train Loss: 0.0093\n",
      "Epoch 2/5, Batch 24/41, Train Loss: 0.0054\n",
      "Epoch 2/5, Batch 32/41, Train Loss: 0.0507\n",
      "Epoch 2/5, Batch 40/41, Train Loss: 0.7692\n",
      "========================================================================================\n",
      "Epoch 2/5, Val Loss: 0.2200, Val Accuracy: 0.9438, Val F1: 0.9428, Best Accuracy: 0.9438\n",
      "========================================================================================\n",
      "Epoch 3/5, Batch 8/41, Train Loss: 0.0127\n",
      "Epoch 3/5, Batch 16/41, Train Loss: 0.0080\n",
      "Epoch 3/5, Batch 24/41, Train Loss: 0.0030\n",
      "Epoch 3/5, Batch 32/41, Train Loss: 0.0014\n",
      "Epoch 3/5, Batch 40/41, Train Loss: 0.0555\n",
      "========================================================================================\n",
      "Epoch 3/5, Val Loss: 0.0309, Val Accuracy: 0.9888, Val F1: 0.9887, Best Accuracy: 0.9888\n",
      "========================================================================================\n",
      "Epoch 4/5, Batch 8/41, Train Loss: 0.0023\n",
      "Epoch 4/5, Batch 16/41, Train Loss: 0.0012\n",
      "Epoch 4/5, Batch 24/41, Train Loss: 0.0007\n",
      "Epoch 4/5, Batch 32/41, Train Loss: 0.0105\n",
      "Epoch 4/5, Batch 40/41, Train Loss: 0.0030\n",
      "========================================================================================\n",
      "Epoch 4/5, Val Loss: 0.3398, Val Accuracy: 0.8876, Val F1: 0.8875, Best Accuracy: 0.9888\n",
      "========================================================================================\n",
      "Epoch 5/5, Batch 8/41, Train Loss: 0.0014\n",
      "Epoch 5/5, Batch 16/41, Train Loss: 0.0007\n",
      "Epoch 5/5, Batch 24/41, Train Loss: 0.0008\n",
      "Epoch 5/5, Batch 32/41, Train Loss: 0.0002\n",
      "Epoch 5/5, Batch 40/41, Train Loss: 0.0002\n",
      "========================================================================================\n",
      "Epoch 5/5, Val Loss: 0.0025, Val Accuracy: 1.0000, Val F1: 1.0000, Best Accuracy: 1.0000\n",
      "========================================================================================\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/workspace/cleft_palate_choja/WAV_PUBLIC_SAMPLES/NOISE\"\n",
    "\n",
    "train_catalog = \"/workspace/cleft_palate_choja/train_noise.csv\"\n",
    "test_catalog = \"/workspace/cleft_palate_choja/test_noise.csv\"\n",
    "model_checkpoint = \"openai/whisper-small\"\n",
    "\n",
    "train_metadata = pd.read_csv(train_catalog)\n",
    "train_df, val_df = train_test_split(train_metadata, test_size = 0.3, random_state = 42)\n",
    "train_files = train_df[\"WAV_filename\"].tolist()\n",
    "train_folder = train_df[\"WAV_folder\"].tolist()\n",
    "train_full_paths = [os.path.join(data_path,train_folder[i], train_files[i]) for i in range(0,len(train_files))]\n",
    "train_labels = train_df[\"hypernasality\"].tolist()\n",
    "# val set\n",
    "val_files = val_df[\"WAV_filename\"].tolist()\n",
    "\n",
    "val_folder = val_df[\"WAV_folder\"].tolist()\n",
    "\n",
    "val_full_paths = [os.path.join(data_path,val_folder[i], val_files[i]) for i in range(0,len(val_files))]\n",
    "\n",
    "val_labels = val_df[\"hypernasality\"].tolist()\n",
    "\n",
    "test_metadata = pd.read_csv(test_catalog)\n",
    "# add cols for wav data\n",
    "\n",
    "# Replace \".mp3\" with \".wav\" in the \"Filename\" column\n",
    "test_metadata['WAV_filename'] = test_metadata['File_Name'].str.replace('.mp3', '.wav')\n",
    "\n",
    "# Create \"WAV_folder\" column by concatenating \"_WAV\" to the \"folder\" column\n",
    "test_metadata['WAV_folder'] = test_metadata['folder'] + \"_WAV\"\n",
    "\n",
    "test_files = test_metadata[\"WAV_filename\"].tolist()\n",
    "\n",
    "test_folder = test_metadata[\"WAV_folder\"].tolist()\n",
    "\n",
    "test_full_paths = [os.path.join(data_path,test_folder[i], test_files[i]) for i in range(0,len(test_files))]\n",
    "\n",
    "#test_full_paths\n",
    "\n",
    "test_labels = test_metadata[\"hypernasality\"].tolist()\n",
    "\n",
    "train_audio_dataset = datasets.Dataset.from_dict({\"audio\": train_full_paths,\n",
    "                                                  \"labels\":train_labels}\n",
    "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "test_audio_dataset = datasets.Dataset.from_dict({\"audio\": test_full_paths,\n",
    "                                                  \"labels\": test_labels}\n",
    "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "val_audio_dataset = datasets.Dataset.from_dict({\"audio\": val_full_paths,\n",
    "                                                 \"labels\": val_labels }\n",
    "                                             ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "#model_checkpoint = \"openai/whisper-base\"\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "encoder = WhisperModel.from_pretrained(model_checkpoint)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class SpeechClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, audio_data,  text_processor):\n",
    "        self.audio_data = audio_data\n",
    "        self.text_processor = text_processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "      inputs = self.text_processor(self.audio_data[index][\"audio\"][\"array\"],\n",
    "                                   return_tensors=\"pt\",\n",
    "                                   sampling_rate=self.audio_data[index][\"audio\"][\"sampling_rate\"])\n",
    "      input_features = inputs.input_features\n",
    "      decoder_input_ids = torch.tensor([[1, 1]]) * encoder.config.decoder_start_token_id\n",
    "\n",
    "      labels = np.array(self.audio_data[index]['labels'])\n",
    "\n",
    "      return input_features, decoder_input_ids, torch.tensor(labels)\n",
    "train_dataset = SpeechClassificationDataset(train_audio_dataset,  feature_extractor)\n",
    "test_dataset = SpeechClassificationDataset(test_audio_dataset,  feature_extractor)\n",
    "val_dataset = SpeechClassificationDataset(val_audio_dataset,  feature_extractor)\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class SpeechClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, encoder):\n",
    "        super(SpeechClassifier, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.encoder.config.hidden_size, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_features, decoder_input_ids):\n",
    "        outputs = self.encoder(input_features, decoder_input_ids=decoder_input_ids)\n",
    "        pooled_output = outputs['last_hidden_state'][:, 0, :]\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "num_labels = 2\n",
    "\n",
    "model = SpeechClassifier(num_labels, encoder).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-08)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Define the training function NO VAL\n",
    "def train(model, train_loader, optimizer, criterion, device, num_epochs):\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "\n",
    "          input_features, decoder_input_ids, labels = batch\n",
    "\n",
    "          input_features = input_features.squeeze()\n",
    "          input_features = input_features.to(device)\n",
    "\n",
    "          decoder_input_ids = decoder_input_ids.squeeze()\n",
    "          decoder_input_ids = decoder_input_ids.to(device)\n",
    "\n",
    "          labels = labels.view(-1)\n",
    "          labels = labels.type(torch.LongTensor)\n",
    "          labels = labels.to(device)\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          logits = model(input_features, decoder_input_ids)\n",
    "\n",
    "          loss = criterion(logits, labels)\n",
    "          loss.backward()\n",
    "\n",
    "          optimizer.step()\n",
    "\n",
    "          if (i+1) % 8 == 0:\n",
    "              print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item():.4f}')\n",
    "\n",
    "    torch.save(model.state_dict(), 'best_model.pt')\n",
    "# Define the training function\n",
    "def train(model, train_loader, val_loader, optimizer,  criterion, device, num_epochs):\n",
    "    best_accuracy = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            input_features, decoder_input_ids, labels = batch\n",
    "            input_features = input_features.squeeze()\n",
    "            input_features = input_features.to(device)\n",
    "            decoder_input_ids = decoder_input_ids.squeeze()\n",
    "            decoder_input_ids = decoder_input_ids.to(device)\n",
    "            labels = labels.view(-1)\n",
    "            labels = labels.type(torch.LongTensor)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_features, decoder_input_ids)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (i+1) % 8 == 0:\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item() :.4f}')\n",
    "                train_loss = 0.0\n",
    "        val_loss, val_accuracy, val_f1, _ , _ = evaluate(model, val_loader, device)\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "        print(\"========================================================================================\")\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}, Best Accuracy: {best_accuracy:.4f}')\n",
    "        print(\"========================================================================================\")\n",
    "def evaluate(model, data_loader,  device):\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "          input_features, decoder_input_ids, labels = batch\n",
    "          input_features = input_features.squeeze()\n",
    "          input_features = input_features.to(device)\n",
    "          decoder_input_ids = decoder_input_ids.squeeze()\n",
    "          decoder_input_ids = decoder_input_ids.to(device)\n",
    "          labels = labels.view(-1)\n",
    "          labels = labels.type(torch.LongTensor)\n",
    "          labels = labels.to(device)\n",
    "          optimizer.zero_grad()\n",
    "          logits = model(input_features, decoder_input_ids)\n",
    "          loss = criterion(logits, labels)\n",
    "          total_loss += loss.item()\n",
    "          _, preds = torch.max(logits, 1)\n",
    "          all_labels.append(labels.cpu().numpy())\n",
    "          all_preds.append(preds.cpu().numpy())\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    loss = total_loss / len(data_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    return loss, accuracy, f1, all_labels, all_preds\n",
    "\n",
    "import librosa\n",
    "num_epochs = 5\n",
    "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Validtation\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        41\n",
      "           1       1.00      1.00      1.00        48\n",
      "\n",
      "    accuracy                           1.00        89\n",
      "   macro avg       1.00      1.00      1.00        89\n",
      "weighted avg       1.00      1.00      1.00        89\n",
      "\n",
      "1.0\n",
      "================\n",
      "Testing\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        36\n",
      "           1       1.00      1.00      1.00        38\n",
      "\n",
      "    accuracy                           1.00        74\n",
      "   macro avg       1.00      1.00      1.00        74\n",
      "weighted avg       1.00      1.00      1.00        74\n",
      "\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#VALIDATION\n",
    "state_dict = torch.load('best_model.pt')\n",
    "\n",
    "# Create a new instance of the model and load the state dictionary\n",
    "num_labels = 2\n",
    "model = SpeechClassifier(num_labels, encoder).to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "_, _, _, all_labels, all_preds = evaluate(model, val_loader, device)\n",
    "\n",
    "#VALIDATION\n",
    "print(\"================\")\n",
    "print(\"Validatation\\n\\n\")\n",
    "print(classification_report(all_labels, all_preds))\n",
    "print(accuracy_score(all_labels, all_preds))\n",
    "\n",
    "# TESTING ONLY\n",
    "state_dict = torch.load('best_model.pt')\n",
    "\n",
    "# Create a new instance of the model and load the state dictionary\n",
    "num_labels = 2\n",
    "model = SpeechClassifier(num_labels, encoder).to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "_, _, _, all_labels, all_preds = evaluate(model, test_loader, device)\n",
    "\n",
    "print(\"================\")\n",
    "print(\"Testing\\n\\n\")\n",
    "print(classification_report(all_labels, all_preds))\n",
    "print(accuracy_score(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==================================================================================================================\n",
    "# Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1499/1211280835.py:26: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test_metadata['WAV_filename'] = test_metadata['File_Name'].str.replace('.mp3', '.wav')\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Batch 8/41, Train Loss: 0.8607\n",
      "Epoch 1/5, Batch 16/41, Train Loss: 0.8422\n",
      "Epoch 1/5, Batch 24/41, Train Loss: 0.5878\n",
      "Epoch 1/5, Batch 32/41, Train Loss: 0.2673\n",
      "Epoch 1/5, Batch 40/41, Train Loss: 0.2562\n",
      "========================================================================================\n",
      "Epoch 1/5, Val Loss: 0.4632, Val Accuracy: 0.7528, Val F1: 0.7456, Best Accuracy: 0.7528\n",
      "========================================================================================\n",
      "Epoch 2/5, Batch 8/41, Train Loss: 0.1017\n",
      "Epoch 2/5, Batch 16/41, Train Loss: 0.0276\n",
      "Epoch 2/5, Batch 24/41, Train Loss: 0.0145\n",
      "Epoch 2/5, Batch 32/41, Train Loss: 0.0206\n",
      "Epoch 2/5, Batch 40/41, Train Loss: 0.2250\n",
      "========================================================================================\n",
      "Epoch 2/5, Val Loss: 0.1881, Val Accuracy: 0.9101, Val F1: 0.9075, Best Accuracy: 0.9101\n",
      "========================================================================================\n",
      "Epoch 3/5, Batch 8/41, Train Loss: 0.0906\n",
      "Epoch 3/5, Batch 16/41, Train Loss: 0.0427\n",
      "Epoch 3/5, Batch 24/41, Train Loss: 0.0074\n",
      "Epoch 3/5, Batch 32/41, Train Loss: 0.1029\n",
      "Epoch 3/5, Batch 40/41, Train Loss: 0.0360\n",
      "========================================================================================\n",
      "Epoch 3/5, Val Loss: 0.0562, Val Accuracy: 0.9888, Val F1: 0.9887, Best Accuracy: 0.9888\n",
      "========================================================================================\n",
      "Epoch 4/5, Batch 8/41, Train Loss: 0.0543\n",
      "Epoch 4/5, Batch 16/41, Train Loss: 0.0332\n",
      "Epoch 4/5, Batch 24/41, Train Loss: 0.0358\n",
      "Epoch 4/5, Batch 32/41, Train Loss: 0.0408\n",
      "Epoch 4/5, Batch 40/41, Train Loss: 0.0207\n",
      "========================================================================================\n",
      "Epoch 4/5, Val Loss: 0.0757, Val Accuracy: 0.9663, Val F1: 0.9662, Best Accuracy: 0.9888\n",
      "========================================================================================\n",
      "Epoch 5/5, Batch 8/41, Train Loss: 0.0137\n",
      "Epoch 5/5, Batch 16/41, Train Loss: 0.0061\n",
      "Epoch 5/5, Batch 24/41, Train Loss: 0.0069\n",
      "Epoch 5/5, Batch 32/41, Train Loss: 0.0115\n",
      "Epoch 5/5, Batch 40/41, Train Loss: 0.4133\n",
      "========================================================================================\n",
      "Epoch 5/5, Val Loss: 0.0190, Val Accuracy: 1.0000, Val F1: 1.0000, Best Accuracy: 1.0000\n",
      "========================================================================================\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/workspace/cleft_palate_choja/WAV_PUBLIC_SAMPLES/NOISE\"\n",
    "\n",
    "train_catalog = \"/workspace/cleft_palate_choja/train_noise.csv\"\n",
    "test_catalog = \"/workspace/cleft_palate_choja/test_noise.csv\"\n",
    "model_checkpoint = \"openai/whisper-medium\"\n",
    "\n",
    "train_metadata = pd.read_csv(train_catalog)\n",
    "train_df, val_df = train_test_split(train_metadata, test_size = 0.3, random_state = 42)\n",
    "train_files = train_df[\"WAV_filename\"].tolist()\n",
    "train_folder = train_df[\"WAV_folder\"].tolist()\n",
    "train_full_paths = [os.path.join(data_path,train_folder[i], train_files[i]) for i in range(0,len(train_files))]\n",
    "train_labels = train_df[\"hypernasality\"].tolist()\n",
    "# val set\n",
    "val_files = val_df[\"WAV_filename\"].tolist()\n",
    "\n",
    "val_folder = val_df[\"WAV_folder\"].tolist()\n",
    "\n",
    "val_full_paths = [os.path.join(data_path,val_folder[i], val_files[i]) for i in range(0,len(val_files))]\n",
    "\n",
    "val_labels = val_df[\"hypernasality\"].tolist()\n",
    "\n",
    "test_metadata = pd.read_csv(test_catalog)\n",
    "# add cols for wav data\n",
    "\n",
    "# Replace \".mp3\" with \".wav\" in the \"Filename\" column\n",
    "test_metadata['WAV_filename'] = test_metadata['File_Name'].str.replace('.mp3', '.wav')\n",
    "\n",
    "# Create \"WAV_folder\" column by concatenating \"_WAV\" to the \"folder\" column\n",
    "test_metadata['WAV_folder'] = test_metadata['folder'] + \"_WAV\"\n",
    "\n",
    "test_files = test_metadata[\"WAV_filename\"].tolist()\n",
    "\n",
    "test_folder = test_metadata[\"WAV_folder\"].tolist()\n",
    "\n",
    "test_full_paths = [os.path.join(data_path,test_folder[i], test_files[i]) for i in range(0,len(test_files))]\n",
    "\n",
    "#test_full_paths\n",
    "\n",
    "test_labels = test_metadata[\"hypernasality\"].tolist()\n",
    "\n",
    "train_audio_dataset = datasets.Dataset.from_dict({\"audio\": train_full_paths,\n",
    "                                                  \"labels\":train_labels}\n",
    "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "test_audio_dataset = datasets.Dataset.from_dict({\"audio\": test_full_paths,\n",
    "                                                  \"labels\": test_labels}\n",
    "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "val_audio_dataset = datasets.Dataset.from_dict({\"audio\": val_full_paths,\n",
    "                                                 \"labels\": val_labels }\n",
    "                                             ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "#model_checkpoint = \"openai/whisper-base\"\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "encoder = WhisperModel.from_pretrained(model_checkpoint)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class SpeechClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, audio_data,  text_processor):\n",
    "        self.audio_data = audio_data\n",
    "        self.text_processor = text_processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "      inputs = self.text_processor(self.audio_data[index][\"audio\"][\"array\"],\n",
    "                                   return_tensors=\"pt\",\n",
    "                                   sampling_rate=self.audio_data[index][\"audio\"][\"sampling_rate\"])\n",
    "      input_features = inputs.input_features\n",
    "      decoder_input_ids = torch.tensor([[1, 1]]) * encoder.config.decoder_start_token_id\n",
    "\n",
    "      labels = np.array(self.audio_data[index]['labels'])\n",
    "\n",
    "      return input_features, decoder_input_ids, torch.tensor(labels)\n",
    "train_dataset = SpeechClassificationDataset(train_audio_dataset,  feature_extractor)\n",
    "test_dataset = SpeechClassificationDataset(test_audio_dataset,  feature_extractor)\n",
    "val_dataset = SpeechClassificationDataset(val_audio_dataset,  feature_extractor)\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class SpeechClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, encoder):\n",
    "        super(SpeechClassifier, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.encoder.config.hidden_size, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_features, decoder_input_ids):\n",
    "        outputs = self.encoder(input_features, decoder_input_ids=decoder_input_ids)\n",
    "        pooled_output = outputs['last_hidden_state'][:, 0, :]\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "num_labels = 2\n",
    "\n",
    "model = SpeechClassifier(num_labels, encoder).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-08)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Define the training function NO VAL\n",
    "def train(model, train_loader, optimizer, criterion, device, num_epochs):\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "\n",
    "          input_features, decoder_input_ids, labels = batch\n",
    "\n",
    "          input_features = input_features.squeeze()\n",
    "          input_features = input_features.to(device)\n",
    "\n",
    "          decoder_input_ids = decoder_input_ids.squeeze()\n",
    "          decoder_input_ids = decoder_input_ids.to(device)\n",
    "\n",
    "          labels = labels.view(-1)\n",
    "          labels = labels.type(torch.LongTensor)\n",
    "          labels = labels.to(device)\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          logits = model(input_features, decoder_input_ids)\n",
    "\n",
    "          loss = criterion(logits, labels)\n",
    "          loss.backward()\n",
    "\n",
    "          optimizer.step()\n",
    "\n",
    "          if (i+1) % 8 == 0:\n",
    "              print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item():.4f}')\n",
    "\n",
    "    torch.save(model.state_dict(), 'best_model.pt')\n",
    "# Define the training function\n",
    "def train(model, train_loader, val_loader, optimizer,  criterion, device, num_epochs):\n",
    "    best_accuracy = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            input_features, decoder_input_ids, labels = batch\n",
    "            input_features = input_features.squeeze()\n",
    "            input_features = input_features.to(device)\n",
    "            decoder_input_ids = decoder_input_ids.squeeze()\n",
    "            decoder_input_ids = decoder_input_ids.to(device)\n",
    "            labels = labels.view(-1)\n",
    "            labels = labels.type(torch.LongTensor)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_features, decoder_input_ids)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (i+1) % 8 == 0:\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item() :.4f}')\n",
    "                train_loss = 0.0\n",
    "        val_loss, val_accuracy, val_f1, _ , _ = evaluate(model, val_loader, device)\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "        print(\"========================================================================================\")\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}, Best Accuracy: {best_accuracy:.4f}')\n",
    "        print(\"========================================================================================\")\n",
    "def evaluate(model, data_loader,  device):\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "          input_features, decoder_input_ids, labels = batch\n",
    "          input_features = input_features.squeeze()\n",
    "          input_features = input_features.to(device)\n",
    "          decoder_input_ids = decoder_input_ids.squeeze()\n",
    "          decoder_input_ids = decoder_input_ids.to(device)\n",
    "          labels = labels.view(-1)\n",
    "          labels = labels.type(torch.LongTensor)\n",
    "          labels = labels.to(device)\n",
    "          optimizer.zero_grad()\n",
    "          logits = model(input_features, decoder_input_ids)\n",
    "          loss = criterion(logits, labels)\n",
    "          total_loss += loss.item()\n",
    "          _, preds = torch.max(logits, 1)\n",
    "          all_labels.append(labels.cpu().numpy())\n",
    "          all_preds.append(preds.cpu().numpy())\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    loss = total_loss / len(data_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    return loss, accuracy, f1, all_labels, all_preds\n",
    "\n",
    "import librosa\n",
    "num_epochs = 5\n",
    "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Validatation\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        41\n",
      "           1       1.00      1.00      1.00        48\n",
      "\n",
      "    accuracy                           1.00        89\n",
      "   macro avg       1.00      1.00      1.00        89\n",
      "weighted avg       1.00      1.00      1.00        89\n",
      "\n",
      "1.0\n",
      "================\n",
      "Testing\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        36\n",
      "           1       1.00      0.95      0.97        38\n",
      "\n",
      "    accuracy                           0.97        74\n",
      "   macro avg       0.97      0.97      0.97        74\n",
      "weighted avg       0.97      0.97      0.97        74\n",
      "\n",
      "0.972972972972973\n"
     ]
    }
   ],
   "source": [
    "#VALIDATION\n",
    "state_dict = torch.load('best_model.pt')\n",
    "\n",
    "# Create a new instance of the model and load the state dictionary\n",
    "num_labels = 2\n",
    "model = SpeechClassifier(num_labels, encoder).to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "_, _, _, all_labels, all_preds = evaluate(model, val_loader, device)\n",
    "\n",
    "#VALIDATION\n",
    "print(\"================\")\n",
    "print(\"Validatation\\n\\n\")\n",
    "print(classification_report(all_labels, all_preds))\n",
    "print(accuracy_score(all_labels, all_preds))\n",
    "\n",
    "# TESTING ONLY\n",
    "state_dict = torch.load('best_model.pt')\n",
    "\n",
    "# Create a new instance of the model and load the state dictionary\n",
    "num_labels = 2\n",
    "model = SpeechClassifier(num_labels, encoder).to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "_, _, _, all_labels, all_preds = evaluate(model, test_loader, device)\n",
    "\n",
    "print(\"================\")\n",
    "print(\"Testing\\n\\n\")\n",
    "print(classification_report(all_labels, all_preds))\n",
    "print(accuracy_score(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==================================================================================================================\n",
    "# Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1499/221531598.py:26: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test_metadata['WAV_filename'] = test_metadata['File_Name'].str.replace('.mp3', '.wav')\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Batch 8/41, Train Loss: 0.7533\n",
      "Epoch 1/5, Batch 16/41, Train Loss: 0.4578\n",
      "Epoch 1/5, Batch 24/41, Train Loss: 0.5272\n",
      "Epoch 1/5, Batch 32/41, Train Loss: 0.1500\n",
      "Epoch 1/5, Batch 40/41, Train Loss: 2.1025\n",
      "========================================================================================\n",
      "Epoch 1/5, Val Loss: 0.7166, Val Accuracy: 0.6966, Val F1: 0.6448, Best Accuracy: 0.6966\n",
      "========================================================================================\n",
      "Epoch 2/5, Batch 8/41, Train Loss: 0.1206\n",
      "Epoch 2/5, Batch 16/41, Train Loss: 0.0607\n",
      "Epoch 2/5, Batch 24/41, Train Loss: 0.0247\n",
      "Epoch 2/5, Batch 32/41, Train Loss: 0.0311\n",
      "Epoch 2/5, Batch 40/41, Train Loss: 0.0240\n",
      "========================================================================================\n",
      "Epoch 2/5, Val Loss: 0.1101, Val Accuracy: 0.9438, Val F1: 0.9438, Best Accuracy: 0.9438\n",
      "========================================================================================\n",
      "Epoch 3/5, Batch 8/41, Train Loss: 0.0216\n",
      "Epoch 3/5, Batch 16/41, Train Loss: 0.0250\n",
      "Epoch 3/5, Batch 24/41, Train Loss: 0.0055\n",
      "Epoch 3/5, Batch 32/41, Train Loss: 0.0047\n",
      "Epoch 3/5, Batch 40/41, Train Loss: 0.0061\n",
      "========================================================================================\n",
      "Epoch 3/5, Val Loss: 0.1294, Val Accuracy: 0.9551, Val F1: 0.9544, Best Accuracy: 0.9551\n",
      "========================================================================================\n",
      "Epoch 4/5, Batch 8/41, Train Loss: 0.0440\n",
      "Epoch 4/5, Batch 16/41, Train Loss: 0.0635\n",
      "Epoch 4/5, Batch 24/41, Train Loss: 0.0504\n",
      "Epoch 4/5, Batch 32/41, Train Loss: 0.0932\n",
      "Epoch 4/5, Batch 40/41, Train Loss: 0.1803\n",
      "========================================================================================\n",
      "Epoch 4/5, Val Loss: 0.2051, Val Accuracy: 0.9213, Val F1: 0.9194, Best Accuracy: 0.9551\n",
      "========================================================================================\n",
      "Epoch 5/5, Batch 8/41, Train Loss: 0.0252\n",
      "Epoch 5/5, Batch 16/41, Train Loss: 0.0364\n",
      "Epoch 5/5, Batch 24/41, Train Loss: 0.0130\n",
      "Epoch 5/5, Batch 32/41, Train Loss: 0.0097\n",
      "Epoch 5/5, Batch 40/41, Train Loss: 0.0050\n",
      "========================================================================================\n",
      "Epoch 5/5, Val Loss: 0.0479, Val Accuracy: 0.9888, Val F1: 0.9887, Best Accuracy: 0.9888\n",
      "========================================================================================\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/workspace/cleft_palate_choja/WAV_PUBLIC_SAMPLES/NOISE\"\n",
    "\n",
    "train_catalog = \"/workspace/cleft_palate_choja/train_noise.csv\"\n",
    "test_catalog = \"/workspace/cleft_palate_choja/test_noise.csv\"\n",
    "model_checkpoint = \"openai/whisper-large-v2\"\n",
    "\n",
    "train_metadata = pd.read_csv(train_catalog)\n",
    "train_df, val_df = train_test_split(train_metadata, test_size = 0.3, random_state = 42)\n",
    "train_files = train_df[\"WAV_filename\"].tolist()\n",
    "train_folder = train_df[\"WAV_folder\"].tolist()\n",
    "train_full_paths = [os.path.join(data_path,train_folder[i], train_files[i]) for i in range(0,len(train_files))]\n",
    "train_labels = train_df[\"hypernasality\"].tolist()\n",
    "# val set\n",
    "val_files = val_df[\"WAV_filename\"].tolist()\n",
    "\n",
    "val_folder = val_df[\"WAV_folder\"].tolist()\n",
    "\n",
    "val_full_paths = [os.path.join(data_path,val_folder[i], val_files[i]) for i in range(0,len(val_files))]\n",
    "\n",
    "val_labels = val_df[\"hypernasality\"].tolist()\n",
    "\n",
    "test_metadata = pd.read_csv(test_catalog)\n",
    "# add cols for wav data\n",
    "\n",
    "# Replace \".mp3\" with \".wav\" in the \"Filename\" column\n",
    "test_metadata['WAV_filename'] = test_metadata['File_Name'].str.replace('.mp3', '.wav')\n",
    "\n",
    "# Create \"WAV_folder\" column by concatenating \"_WAV\" to the \"folder\" column\n",
    "test_metadata['WAV_folder'] = test_metadata['folder'] + \"_WAV\"\n",
    "\n",
    "test_files = test_metadata[\"WAV_filename\"].tolist()\n",
    "\n",
    "test_folder = test_metadata[\"WAV_folder\"].tolist()\n",
    "\n",
    "test_full_paths = [os.path.join(data_path,test_folder[i], test_files[i]) for i in range(0,len(test_files))]\n",
    "\n",
    "#test_full_paths\n",
    "\n",
    "test_labels = test_metadata[\"hypernasality\"].tolist()\n",
    "\n",
    "train_audio_dataset = datasets.Dataset.from_dict({\"audio\": train_full_paths,\n",
    "                                                  \"labels\":train_labels}\n",
    "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "test_audio_dataset = datasets.Dataset.from_dict({\"audio\": test_full_paths,\n",
    "                                                  \"labels\": test_labels}\n",
    "                                                 ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "val_audio_dataset = datasets.Dataset.from_dict({\"audio\": val_full_paths,\n",
    "                                                 \"labels\": val_labels }\n",
    "                                             ).cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "#model_checkpoint = \"openai/whisper-base\"\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "encoder = WhisperModel.from_pretrained(model_checkpoint)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class SpeechClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, audio_data,  text_processor):\n",
    "        self.audio_data = audio_data\n",
    "        self.text_processor = text_processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "      inputs = self.text_processor(self.audio_data[index][\"audio\"][\"array\"],\n",
    "                                   return_tensors=\"pt\",\n",
    "                                   sampling_rate=self.audio_data[index][\"audio\"][\"sampling_rate\"])\n",
    "      input_features = inputs.input_features\n",
    "      decoder_input_ids = torch.tensor([[1, 1]]) * encoder.config.decoder_start_token_id\n",
    "\n",
    "      labels = np.array(self.audio_data[index]['labels'])\n",
    "\n",
    "      return input_features, decoder_input_ids, torch.tensor(labels)\n",
    "train_dataset = SpeechClassificationDataset(train_audio_dataset,  feature_extractor)\n",
    "test_dataset = SpeechClassificationDataset(test_audio_dataset,  feature_extractor)\n",
    "val_dataset = SpeechClassificationDataset(val_audio_dataset,  feature_extractor)\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class SpeechClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, encoder):\n",
    "        super(SpeechClassifier, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.encoder.config.hidden_size, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_features, decoder_input_ids):\n",
    "        outputs = self.encoder(input_features, decoder_input_ids=decoder_input_ids)\n",
    "        pooled_output = outputs['last_hidden_state'][:, 0, :]\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "num_labels = 2\n",
    "\n",
    "model = SpeechClassifier(num_labels, encoder).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-08)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Define the training function NO VAL\n",
    "def train(model, train_loader, optimizer, criterion, device, num_epochs):\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "\n",
    "          input_features, decoder_input_ids, labels = batch\n",
    "\n",
    "          input_features = input_features.squeeze()\n",
    "          input_features = input_features.to(device)\n",
    "\n",
    "          decoder_input_ids = decoder_input_ids.squeeze()\n",
    "          decoder_input_ids = decoder_input_ids.to(device)\n",
    "\n",
    "          labels = labels.view(-1)\n",
    "          labels = labels.type(torch.LongTensor)\n",
    "          labels = labels.to(device)\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          logits = model(input_features, decoder_input_ids)\n",
    "\n",
    "          loss = criterion(logits, labels)\n",
    "          loss.backward()\n",
    "\n",
    "          optimizer.step()\n",
    "\n",
    "          if (i+1) % 8 == 0:\n",
    "              print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item():.4f}')\n",
    "\n",
    "    torch.save(model.state_dict(), 'best_model.pt')\n",
    "# Define the training function\n",
    "def train(model, train_loader, val_loader, optimizer,  criterion, device, num_epochs):\n",
    "    best_accuracy = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            input_features, decoder_input_ids, labels = batch\n",
    "            input_features = input_features.squeeze()\n",
    "            input_features = input_features.to(device)\n",
    "            decoder_input_ids = decoder_input_ids.squeeze()\n",
    "            decoder_input_ids = decoder_input_ids.to(device)\n",
    "            labels = labels.view(-1)\n",
    "            labels = labels.type(torch.LongTensor)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_features, decoder_input_ids)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (i+1) % 8 == 0:\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item() :.4f}')\n",
    "                train_loss = 0.0\n",
    "        val_loss, val_accuracy, val_f1, _ , _ = evaluate(model, val_loader, device)\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "        print(\"========================================================================================\")\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}, Best Accuracy: {best_accuracy:.4f}')\n",
    "        print(\"========================================================================================\")\n",
    "def evaluate(model, data_loader,  device):\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "          input_features, decoder_input_ids, labels = batch\n",
    "          input_features = input_features.squeeze()\n",
    "          input_features = input_features.to(device)\n",
    "          decoder_input_ids = decoder_input_ids.squeeze()\n",
    "          decoder_input_ids = decoder_input_ids.to(device)\n",
    "          labels = labels.view(-1)\n",
    "          labels = labels.type(torch.LongTensor)\n",
    "          labels = labels.to(device)\n",
    "          optimizer.zero_grad()\n",
    "          logits = model(input_features, decoder_input_ids)\n",
    "          loss = criterion(logits, labels)\n",
    "          total_loss += loss.item()\n",
    "          _, preds = torch.max(logits, 1)\n",
    "          all_labels.append(labels.cpu().numpy())\n",
    "          all_preds.append(preds.cpu().numpy())\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    loss = total_loss / len(data_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    return loss, accuracy, f1, all_labels, all_preds\n",
    "\n",
    "import librosa\n",
    "num_epochs = 5\n",
    "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Validatation\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        41\n",
      "           1       0.98      1.00      0.99        48\n",
      "\n",
      "    accuracy                           0.99        89\n",
      "   macro avg       0.99      0.99      0.99        89\n",
      "weighted avg       0.99      0.99      0.99        89\n",
      "\n",
      "0.9887640449438202\n",
      "================\n",
      "Testing\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95        36\n",
      "           1       1.00      0.89      0.94        38\n",
      "\n",
      "    accuracy                           0.95        74\n",
      "   macro avg       0.95      0.95      0.95        74\n",
      "weighted avg       0.95      0.95      0.95        74\n",
      "\n",
      "0.9459459459459459\n"
     ]
    }
   ],
   "source": [
    "#VALIDATION\n",
    "state_dict = torch.load('best_model.pt')\n",
    "\n",
    "# Create a new instance of the model and load the state dictionary\n",
    "num_labels = 2\n",
    "model = SpeechClassifier(num_labels, encoder).to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "_, _, _, all_labels, all_preds = evaluate(model, val_loader, device)\n",
    "\n",
    "#VALIDATION\n",
    "print(\"================\")\n",
    "print(\"Validatation\\n\\n\")\n",
    "print(classification_report(all_labels, all_preds))\n",
    "print(accuracy_score(all_labels, all_preds))\n",
    "\n",
    "# TESTING ONLY\n",
    "state_dict = torch.load('best_model.pt')\n",
    "\n",
    "# Create a new instance of the model and load the state dictionary\n",
    "num_labels = 2\n",
    "model = SpeechClassifier(num_labels, encoder).to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "_, _, _, all_labels, all_preds = evaluate(model, test_loader, device)\n",
    "\n",
    "print(\"================\")\n",
    "print(\"Testing\\n\\n\")\n",
    "print(classification_report(all_labels, all_preds))\n",
    "print(accuracy_score(all_labels, all_preds))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "262f00dd95404da08656c09e11343d5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_741790bd41ed49da82cc65329d5420cf",
       "IPY_MODEL_b9dc8ca1c66e4ed8813fea0ee6b5013c",
       "IPY_MODEL_edf13b3438b94ad4a46801585ce2e7ed"
      ],
      "layout": "IPY_MODEL_cb4e2ce75af74eefa2cc9dea57a28af6"
     }
    },
    "3b988137321345ab9d5053fa3bb1584a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ecfecbb0c564add9c7e0d934d36cf4f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40d34a6291c1425bb189d39786a1d9e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4f3cb021985744b0ad19ae7cf39ca1a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "520c0a3e5b904aa1b253af945977bbac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6b161a029f4a4319bc03486aa6095418": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6de39f1a49684c6b9e4e6f590a753167": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6f7a9ecfc73446f3aed7264cca40b513": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cc031250814b4b80a623ed770d26501f",
      "max": 184990,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fea45e76479747c782c1501e7fb2df40",
      "value": 184990
     }
    },
    "730678f8b7b94787a92baff44328b967": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "73f88ec9be55418495b8a6d18caaf309": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "741790bd41ed49da82cc65329d5420cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be214894409c41dba66907d2e8bbbe3f",
      "placeholder": "",
      "style": "IPY_MODEL_730678f8b7b94787a92baff44328b967",
      "value": "model.safetensors:100%"
     }
    },
    "75680f71cc9b4c50aa5d037c11a5d4cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8a0a9b46f37d45bd8fe9f25cfad4c2e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bf9e4d0c7c7d42f490ec00cb01388780",
      "placeholder": "",
      "style": "IPY_MODEL_f26c492caee849109f16a8739a094478",
      "value": "config.json:100%"
     }
    },
    "8aa0332a573949248a31446990eeea8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8a0a9b46f37d45bd8fe9f25cfad4c2e6",
       "IPY_MODEL_d9981e0dd6934118b82dc7ee3af2b151",
       "IPY_MODEL_d88980a81c9846b4b29536cef16f7431"
      ],
      "layout": "IPY_MODEL_c2e0c28aa38148ce8cda7959ce4e9a31"
     }
    },
    "8b44bf9bd55c4510b13b679850000fc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b988137321345ab9d5053fa3bb1584a",
      "placeholder": "",
      "style": "IPY_MODEL_6de39f1a49684c6b9e4e6f590a753167",
      "value": "185k/185k[00:00&lt;00:00,430kB/s]"
     }
    },
    "8cb9edfc75434d14b838b412c6810196": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a10c743f1fb14855b87d4be1776624ba",
       "IPY_MODEL_6f7a9ecfc73446f3aed7264cca40b513",
       "IPY_MODEL_8b44bf9bd55c4510b13b679850000fc2"
      ],
      "layout": "IPY_MODEL_73f88ec9be55418495b8a6d18caaf309"
     }
    },
    "97ef9e98800047c9928b7e818df16628": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a10c743f1fb14855b87d4be1776624ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6b161a029f4a4319bc03486aa6095418",
      "placeholder": "",
      "style": "IPY_MODEL_4f3cb021985744b0ad19ae7cf39ca1a9",
      "value": "preprocessor_config.json:100%"
     }
    },
    "a9771fb56dda4e85a03903528a99db23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b9dc8ca1c66e4ed8813fea0ee6b5013c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb3e295db229449da0f1db045ee8ab9d",
      "max": 290403936,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a9771fb56dda4e85a03903528a99db23",
      "value": 290403936
     }
    },
    "be214894409c41dba66907d2e8bbbe3f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf9e4d0c7c7d42f490ec00cb01388780": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2e0c28aa38148ce8cda7959ce4e9a31": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb4e2ce75af74eefa2cc9dea57a28af6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc031250814b4b80a623ed770d26501f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d88980a81c9846b4b29536cef16f7431": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e29f8e6a1aa242fd8693bcf6db07908f",
      "placeholder": "",
      "style": "IPY_MODEL_75680f71cc9b4c50aa5d037c11a5d4cb",
      "value": "1.98k/1.98k[00:00&lt;00:00,131kB/s]"
     }
    },
    "d9981e0dd6934118b82dc7ee3af2b151": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_97ef9e98800047c9928b7e818df16628",
      "max": 1983,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_520c0a3e5b904aa1b253af945977bbac",
      "value": 1983
     }
    },
    "e29f8e6a1aa242fd8693bcf6db07908f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "edf13b3438b94ad4a46801585ce2e7ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ecfecbb0c564add9c7e0d934d36cf4f",
      "placeholder": "",
      "style": "IPY_MODEL_40d34a6291c1425bb189d39786a1d9e3",
      "value": "290M/290M[00:04&lt;00:00,44.9MB/s]"
     }
    },
    "f26c492caee849109f16a8739a094478": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fb3e295db229449da0f1db045ee8ab9d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fea45e76479747c782c1501e7fb2df40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
