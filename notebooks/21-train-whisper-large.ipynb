{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyB0fe7yF4Mh"
      },
      "source": [
        "# Fine-tuning Whisper on Speech Pathology Dataset\n",
        "\n",
        "## Goal\n",
        "\n",
        "The goal of the Cleft Palate project (name TBD) at Vanderbilt DSI is to classify audio clips of patients' voices as containing hypernasality (a speech impediment) or not. The patients with hypernasality can then be recommended for speech pathology intervention. This is currently evaluated by human speech pathologists, which requires access to these medical providers. Our hope is to train a model that can classify this speech impediment for expedited patient access to a speech pathologist."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-jy3au5F4Mi"
      },
      "source": [
        "## Model\n",
        "\n",
        "In this notebook we train the Whisper model with a Sequence Classification Head."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZrXyRlPF4Mi"
      },
      "source": [
        "## Data\n",
        "\n",
        "The data in this notebook is publicly available voice recordings featuring hypernasality and control groups. In the future we hope to train our model on private patient data from Vanderbilt University Medical Center (VUMC)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KifWdFjsF-B6"
      },
      "outputs": [],
      "source": [
        "# FOR GOOGLE DRIVE USE ONLY\n",
        "!pip install torch\n",
        "!pip install datasets\n",
        "!pip install librosa\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0frbPwrsF4Mj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\useltom\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# import libraries\n",
        "import datasets\n",
        "from datasets import load_dataset, DatasetDict,  Audio\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import librosa\n",
        "import io\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
        "from transformers import WhisperModel, WhisperFeatureExtractor, AdamW\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset, load_from_disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_KbJHQxHJyp",
        "outputId": "8bba79e7-6884-4189-b769-daec6416f50b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# mount google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSk_DTreF4Ml"
      },
      "source": [
        "### Load PyTorch datasets\n",
        "\n",
        "If you have not already, run the `01-load-data.ipynb` notebook to create the files \"../data/train_dataset.pt\", \"../data/val_dataset.pt\", and \"../data/test_dataset.pt\". Then, load them using the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "exgEfm5JT0qm"
      },
      "outputs": [],
      "source": [
        "# load data from disk\n",
        "train_audio_dataset = load_from_disk('../data/public_samples/train_dataset')\n",
        "test_audio_dataset = load_from_disk('../data/public_samples/test_dataset')\n",
        "val_audio_dataset = load_from_disk('../data/public_samples/val_dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cC0SCQsZF4Mm"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "858df34808d548e38064d6468f13a5b7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\useltom\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\useltom\\.cache\\huggingface\\hub\\models--openai--whisper-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b40dc285a86b40ec8ae57d2cb38761f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.98k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b568475522f8432084f1037ca833bc78",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/290M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_checkpoint = \"openai/whisper-large-v2\"\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_checkpoint)\n",
        "encoder = WhisperModel.from_pretrained(model_checkpoint)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "09pDF62uF4Mm"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SpeechClassificationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, audio_data,  text_processor):\n",
        "        self.audio_data = audio_data\n",
        "        self.text_processor = text_processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "      inputs = self.text_processor(self.audio_data[index][\"audio\"][\"array\"],\n",
        "                                   return_tensors=\"pt\",\n",
        "                                   sampling_rate=self.audio_data[index][\"audio\"][\"sampling_rate\"])\n",
        "      input_features = inputs.input_features\n",
        "      decoder_input_ids = torch.tensor([[1, 1]]) * encoder.config.decoder_start_token_id\n",
        "\n",
        "      labels = np.array(self.audio_data[index]['labels'])\n",
        "\n",
        "      return input_features, decoder_input_ids, torch.tensor(labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "5gw-7620F4Mm"
      },
      "outputs": [],
      "source": [
        "train_dataset = SpeechClassificationDataset(train_audio_dataset,  feature_extractor)\n",
        "test_dataset = SpeechClassificationDataset(test_audio_dataset,  feature_extractor)\n",
        "val_dataset = SpeechClassificationDataset(val_audio_dataset,  feature_extractor)\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzARqHUEF4Mm"
      },
      "source": [
        "## Training\n",
        "\n",
        "Training for Whisper model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JVjXzUnbF4Mm"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SpeechClassifier(nn.Module):\n",
        "    def __init__(self, num_labels, encoder):\n",
        "        super(SpeechClassifier, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.encoder.config.hidden_size, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4096, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_features, decoder_input_ids):\n",
        "        outputs = self.encoder(input_features, decoder_input_ids=decoder_input_ids)\n",
        "        pooled_output = outputs['last_hidden_state'][:, 0, :]\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIkVorEXF4Mn",
        "outputId": "6a6b4e91-a191-46c3-efcc-19dc9b52b1f8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "num_labels = 2\n",
        "\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-08)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ziIJ-x0BF4Mn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the training function\n",
        "def train(model, train_loader, val_loader, optimizer,  criterion, device, num_epochs):\n",
        "\n",
        "    best_accuracy = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for i, batch in enumerate(train_loader):\n",
        "\n",
        "            input_features, decoder_input_ids, labels = batch\n",
        "\n",
        "            input_features = input_features.squeeze()\n",
        "            input_features = input_features.to(device)\n",
        "\n",
        "            decoder_input_ids = decoder_input_ids.squeeze()\n",
        "            decoder_input_ids = decoder_input_ids.to(device)\n",
        "\n",
        "            labels = labels.view(-1)\n",
        "            labels = labels.type(torch.LongTensor)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits = model(input_features, decoder_input_ids)\n",
        "\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            if (i+1) % 8 == 0:\n",
        "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item() :.4f}')\n",
        "                train_loss = 0.0\n",
        "\n",
        "        val_loss, val_accuracy, val_f1, _ , _ = evaluate(model, val_loader, device)\n",
        "\n",
        "        if val_accuracy > best_accuracy:\n",
        "            best_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), 'whisper_large_best_model.pt')\n",
        "\n",
        "        print(\"========================================================================================\")\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}, Best Accuracy: {best_accuracy:.4f}')\n",
        "        print(\"========================================================================================\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Z7Q4-ze0F4Mn"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data_loader,  device):\n",
        "\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, batch in enumerate(data_loader):\n",
        "\n",
        "          input_features, decoder_input_ids, labels = batch\n",
        "\n",
        "          input_features = input_features.squeeze()\n",
        "          input_features = input_features.to(device)\n",
        "\n",
        "          decoder_input_ids = decoder_input_ids.squeeze()\n",
        "          decoder_input_ids = decoder_input_ids.to(device)\n",
        "\n",
        "          labels = labels.view(-1)\n",
        "          labels = labels.type(torch.LongTensor)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          logits = model(input_features, decoder_input_ids)\n",
        "\n",
        "          loss = criterion(logits, labels)\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          _, preds = torch.max(logits, 1)\n",
        "          all_labels.append(labels.cpu().numpy())\n",
        "          all_preds.append(preds.cpu().numpy())\n",
        "\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "\n",
        "    loss = total_loss / len(data_loader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return loss, accuracy, f1, all_labels, all_preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lu7htLYPE_9h"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUv6LcFNF4Mn",
        "outputId": "f92a0109-8d1a-4cf2-89e7-cbf354b1d859"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Batch 8/19, Train Loss: 0.6754\n",
            "Epoch 1/5, Batch 16/19, Train Loss: 0.5096\n",
            "========================================================================================\n",
            "Epoch 1/5, Val Loss: 0.2350, Val Accuracy: 0.9556, Val F1: 0.9554, Best Accuracy: 0.9556\n",
            "========================================================================================\n",
            "Epoch 2/5, Batch 8/19, Train Loss: 0.0305\n",
            "Epoch 2/5, Batch 16/19, Train Loss: 0.0067\n",
            "========================================================================================\n",
            "Epoch 2/5, Val Loss: 0.0244, Val Accuracy: 0.9778, Val F1: 0.9777, Best Accuracy: 0.9778\n",
            "========================================================================================\n",
            "Epoch 3/5, Batch 8/19, Train Loss: 0.0070\n",
            "Epoch 3/5, Batch 16/19, Train Loss: 0.0028\n",
            "========================================================================================\n",
            "Epoch 3/5, Val Loss: 0.1729, Val Accuracy: 0.9556, Val F1: 0.9555, Best Accuracy: 0.9778\n",
            "========================================================================================\n",
            "Epoch 4/5, Batch 8/19, Train Loss: 0.0006\n",
            "Epoch 4/5, Batch 16/19, Train Loss: 0.0517\n",
            "========================================================================================\n",
            "Epoch 4/5, Val Loss: 0.1358, Val Accuracy: 0.9778, Val F1: 0.9777, Best Accuracy: 0.9778\n",
            "========================================================================================\n",
            "Epoch 5/5, Batch 8/19, Train Loss: 0.0002\n",
            "Epoch 5/5, Batch 16/19, Train Loss: 0.0005\n",
            "========================================================================================\n",
            "Epoch 5/5, Val Loss: 0.9425, Val Accuracy: 0.8667, Val F1: 0.8650, Best Accuracy: 0.9778\n",
            "========================================================================================\n"
          ]
        }
      ],
      "source": [
        "import librosa\n",
        "num_epochs = 5\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjETS9sqYRD5"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "#### Validation Set\n",
        "\n",
        "Evaluate how our model performs on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "bILZRSn4JvlG"
      },
      "outputs": [],
      "source": [
        "#VALIDATION\n",
        "state_dict = torch.load('whisper_large_best_model.pt')\n",
        "\n",
        "# Create a new instance of the model and load the state dictionary\n",
        "num_labels = 2\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "_, _, _, all_labels, all_preds = evaluate(model, val_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyNUpLHeJ5p1",
        "outputId": "52379010-0e14-4cd5-eaa7-bd858a033532"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.95      0.98        22\n",
            "           1       0.96      1.00      0.98        23\n",
            "\n",
            "    accuracy                           0.98        45\n",
            "   macro avg       0.98      0.98      0.98        45\n",
            "weighted avg       0.98      0.98      0.98        45\n",
            "\n",
            "0.9777777777777777\n"
          ]
        }
      ],
      "source": [
        "#VALIDATION\n",
        "print(classification_report(all_labels, all_preds))\n",
        "print(accuracy_score(all_labels, all_preds))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
